{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c6d01b4",
   "metadata": {},
   "source": [
    "# Báo cáo đồ án môn \"Khoa học dữ liệu ứng dụng\"\n",
    "\n",
    "Nhóm 11:\n",
    "1. 1612355 - Lê Kinh Luân - https://github.com/kluan98\n",
    "2. 1712284 - Hoàng Gia Bảo - https://github.com/hgbao8799\n",
    "3. 18120052 - Lê Hạnh Linh - https://github.com/linh0303052\n",
    "4. 18120182 - Lê Hồng Huy - https://github.com/lehonghuy2000\n",
    "5. 1712371 - Nguyễn Văn Dưng - https://github.com/dungdev1\n",
    "\n",
    "Link thùng chứa Github của nhóm: https://github.com/linh0303052/AplliedDSGroup11"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "086a68ca",
   "metadata": {},
   "source": [
    "## Mô tả bài toán"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b6bbc0",
   "metadata": {},
   "source": [
    "Bài toán **Santander Customer Satisfaction** được tổ chức bởi Ngân hàng Santander có thị trường là Đông Bắc Hoa Kỳ và là công ty con của tập đoàn Santander ở Tây Ban Nha.\n",
    "\n",
    "**Link:** https://www.kaggle.com/c/santander-customer-satisfaction/overview\n",
    "\n",
    "Vấn đề: Phân tích sự hài lòng của khách hàng là phép đo chìa khóa của thành công bởi vì các khách hàng không hài lòng sẽ có ít cơ hội mua lại lần tiếp theo và quan trọng hơn là họ sẽ nói lên sự không hài lòng của họ trên các nền tảng xã hội trước khi rời đi. Ngân hàng Santander mong muốn phát hiện sớm các khách hàng không hài lòng và có các giải pháp chủ động để cải thiện trải nghiệm, thái độ của khách hàng trước khi quá muộn.\n",
    "\n",
    "**Input:** Dữ liệu gồm 370 thuộc tính được anonymized\n",
    "\n",
    "**Output:** Dự đoán đây có phải là khách hàng không hài lòng với kết quả là 1 nếu *không hài lòng* hoặc 0 nếu *hài lòng*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b9a725",
   "metadata": {},
   "source": [
    "## Giải quyết bài toán"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afca1d13",
   "metadata": {},
   "source": [
    "Trong đồ án này, nhóm tìm hiểu và cài đặt lại phương pháp của nhóm **#1 Leustagos** đặt hạng 3 trên Private Leaderboard\n",
    "\n",
    "**Github:** https://github.com/diefimov/santander_2016\n",
    "\n",
    "**Mô tả:** Giải pháp của tác giả chủ yếu dựa trên kết hợp (ensemble) của các mô hình khác nhau vì dataset gồm rất nhiều thuộc tính nên việc dùng ensemble để tránh tình trạng overfit. Các tác giả sử dụng khá đa dạng các mô hình có tính chất làm việc tốt trên dataset bị ẩn danh hóa và dạng bảng như Random Forest, Adaboost Classifier, XGBOOST, Neural Network. Mỗi nhóm mô hình có cách tiền xử lý dữ liệu khác nhau dành cho mỗi mô hình thay vì chỉ tiền xử lý một lần đầu và dùng cho tất cả mô hình. Đây là lí do nhóm chọn giải pháp của nhóm tác giả này trong số các giải pháp top đầu.\n",
    "\n",
    "Giải pháp này là một ensemble của 5 nhóm mô hình lớn, được tiền xử lý và chạy độc lập với nhau. Đặc điểm này cũng quy định cấu trúc của phần tiếp theo: mỗi nhóm mô hình sẽ được chia thành 2 phần lớn là phần *tiền xử lý dữ liệu* và phần *mô hình*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15bd175d",
   "metadata": {},
   "source": [
    "### Import các thư viện cần thiết"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3879827e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from scipy.stats.stats import pearsonr\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import Normalizer, PolynomialFeatures, MinMaxScaler\n",
    "from datetime import date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1104508b",
   "metadata": {},
   "source": [
    "### Tải dữ liệu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8e7ce0f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(76020, 371)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('./data/input/train.csv')\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "efd6ab22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(75818, 370)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.read_csv('./data/input/test.csv')\n",
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9b0deb61",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_PATH = './data/input/'\n",
    "OUTPUT_PATH = './data/output/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45cb3ce",
   "metadata": {},
   "source": [
    "### Nhóm mô hình Dmitry Efimov"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84efa31",
   "metadata": {},
   "source": [
    "#### Tiền xử lý dữ liệu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "dd93581c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#xử lý các giá trị đặc biệt, thay thế nó bằng giá trị NA (-999.0)\n",
    "train.loc[(train['var38']>117310.979) & (train['var38']<117310.98), 'var38'] = -999.0\n",
    "test.loc[(test['var38']>117310.979) & (test['var38']<117310.98), 'var38'] = -999.0\n",
    "\n",
    "train.loc[train['var3']==-999999, 'var3'] = -999.0\n",
    "test.loc[test['var3']==-999999, 'var3'] = -999.0\n",
    "\n",
    "for f in ['imp_op_var40_comer_ult1', 'imp_op_var40_efect_ult3', 'imp_op_var41_comer_ult3', 'imp_sal_var16_ult1']:\n",
    "    train.loc[train[f]==0.0, f] = -999.0\n",
    "    test.loc[test[f]==0.0, f] = -999.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9699563f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loại bỏ các thuộc tính chỉ có 1 giá trị\n",
    "flist = [x for x in train.columns if not x in ['ID','TARGET']]\n",
    "for f in flist:\n",
    "    if len(np.unique(train[f]))<2:\n",
    "        train.drop(f, axis=1, inplace=True)\n",
    "        test.drop(f, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "75a5505b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loại bỏ var6 vì nó trùng với var29\n",
    "flist = [x for x in train.columns if not x in ['ID','TARGET']]            \n",
    "train.drop([x for x in flist if 'var6' in x], axis=1, inplace=True)\n",
    "test.drop([x for x in flist if 'var6' in x], axis=1, inplace=True)\n",
    "\n",
    "#Loại bỏ các thuộc tính có chứa _0 vì nó bị trùng với cột có chứa _1 theo ngay sau\n",
    "flist = [x for x in train.columns if not x in ['ID','TARGET']]        \n",
    "flist_remove = []\n",
    "for i in range(len(flist)-1):\n",
    "    v = train[flist[i]].values\n",
    "    for j in range(i+1, len(flist)):\n",
    "        if np.array_equal(v, train[flist[j]].values):\n",
    "            if '_0' in flist[j]:\n",
    "                flist_remove.append(flist[j])\n",
    "            elif  '_0' in flist[i]:\n",
    "                flist_remove.append(flist[i])\n",
    "train.drop(flist_remove, axis=1, inplace=True)\n",
    "test.drop(flist_remove, axis=1, inplace=True)\n",
    "\n",
    "#Loại bỏ các cột bị trùng khác\n",
    "flist_remove = ['saldo_medio_var13_medio_ult1', 'delta_imp_reemb_var13_1y3', 'delta_imp_reemb_var17_1y3', \n",
    "                   'delta_imp_reemb_var33_1y3', 'delta_imp_trasp_var17_in_1y3', 'delta_imp_trasp_var17_out_1y3',\n",
    "                   'delta_imp_trasp_var33_in_1y3', 'delta_imp_trasp_var33_out_1y3']\n",
    "train.drop(flist_remove, axis=1, inplace=True)\n",
    "test.drop(flist_remove, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69428089",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Chuẩn hóa các giá trị thuộc tính\n",
    "def normalize_features(train, test):\n",
    "    flist = [x for x in train.columns if not x in ['ID','TARGET']]\n",
    "    for f in flist:\n",
    "        if train[f].max() == 9999999999.0:\n",
    "            fmax = train.loc[train[f]<9999999999.0, f].max()\n",
    "            train.loc[train[f]==9999999999.0, f] = fmax + 1\n",
    "\n",
    "        if len(train.loc[train[f]<0, f].value_counts()) == 1:\n",
    "            train.loc[train[f]<0, f] = -1.0\n",
    "            test.loc[test[f]<0, f] = -1.0\n",
    "            fmax = max(np.max(train[f]), np.max(test[f]))\n",
    "            if fmax > 0:\n",
    "                train.loc[train[f]>0, f] = 1.0*train.loc[train[f]>0, f]/fmax\n",
    "                test.loc[test[f]>0, f] = 1.0*test.loc[test[f]>0, f]/fmax\n",
    "\n",
    "        if len(train.loc[train[f]<0, f]) == 0:\n",
    "            fmax = max(np.max(train[f]), np.max(test[f]))\n",
    "            if fmax > 0:\n",
    "                train.loc[train[f]>0, f] = 1.0*train.loc[train[f]>0, f]/fmax\n",
    "                test.loc[test[f]>0, f] = 1.0*test.loc[test[f]>0, f]/fmax\n",
    "\n",
    "        if len(train.loc[train[f]<0, f].value_counts()) > 1:\n",
    "            fmax = max(np.max(train[f]), np.max(test[f]))\n",
    "            if fmax > 0:\n",
    "                train[f] = 1.0*train[f]/fmax\n",
    "                test[f] = 1.0*test[f]/fmax\n",
    "\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22ced5c",
   "metadata": {},
   "source": [
    "#### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e3918e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#thuộc tính t_SNE\n",
    "np.random.seed(12324)\n",
    "train_tsne, test_tsne = add_features(train, test, ['SumZeros'])\n",
    "\n",
    "flist = [x for x in train_tsne.columns if not x in ['ID','TARGET']]\n",
    "\n",
    "X = train_tsne[flist].append(test[flist], ignore_index=True).values.astype('float64')\n",
    "svd = TruncatedSVD(n_components=30)\n",
    "X_svd = svd.fit_transform(X)\n",
    "X_scaled = StandardScaler().fit_transform(X_svd)\n",
    "feats_tsne = TSNE(n_components=2, random_state=0).fit_transform(X_scaled)\n",
    "feats_tsne = pd.DataFrame(feats_tsne, columns=['tsne1', 'tsne2'])\n",
    "feats_tsne['ID'] = train_tsne[['ID']].append(test_tsne[['ID']], ignore_index=True)['ID'].values\n",
    "train_tsne = pd.merge(train_tsne, feats_tsne, on='ID', how='left')\n",
    "test_tsne = pd.merge(test_tsne, feats_tsne, on='ID', how='left')\n",
    "\n",
    "feat = train[['ID', 'tsne1', 'tsne2']].append(test[['ID', 'tsne1', 'tsne2']], ignore_index=True)\n",
    "feat.to_csv(OUTPUT_PATH + 'tsne_feats.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe01300",
   "metadata": {},
   "outputs": [],
   "source": [
    "#thuộc tính PCA\n",
    "train_pca, test_pca = add_features(train, test, ['SumZeros'])\n",
    "\n",
    "flist = [x for x in train_pca.columns if not x in ['ID','TARGET']]\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "x_train_projected = pca.fit_transform(normalize(train_pca[flist], axis=0))\n",
    "x_test_projected = pca.transform(normalize(test_pca[flist], axis=0))\n",
    "train_pca.insert(1, 'PCAOne', x_train_projected[:, 0])\n",
    "train_pca.insert(1, 'PCATwo', x_train_projected[:, 1])\n",
    "test_pca.insert(1, 'PCAOne', x_test_projected[:, 0])\n",
    "test_pca.insert(1, 'PCATwo', x_test_projected[:, 1])\n",
    "pca_feats = train_pca[['ID', 'PCAOne', 'PCATwo']].append(test_pca[['ID', 'PCAOne', 'PCATwo']], ignore_index=True)\n",
    "pca_feats.to_csv(OUTPUT_PATH + 'dmitry_pca_feats.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cafcb43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#thuộc tính k-means\n",
    "train_k, test_k = add_features(train_k, test, ['SumZeros'])\n",
    "train_k, test_k = normalize_features(train_k, test)\n",
    "\n",
    "flist = [x for x in train_k.columns if not x in ['ID','TARGET']]\n",
    "\n",
    "flist_kmeans = []\n",
    "for ncl in range(2,11):\n",
    "    cls = KMeans(n_clusters=ncl)\n",
    "    cls.fit_predict(train_k[flist].values)\n",
    "    train_k['kmeans_cluster'+str(ncl)] = cls.predict(train_k[flist].values)\n",
    "    test_k['kmeans_cluster'+str(ncl)] = cls.predict(test_k[flist].values)\n",
    "    flist_kmeans.append('kmeans_cluster'+str(ncl))\n",
    "\n",
    "train[['ID']+flist_kmeans].append(test[['ID']+flist_kmeans], ignore_index=True).to_csv(OUTPUT_PATH + 'kmeans_feats.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3b7c699f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LL = (30.yTB + yG)/(30 + |G|)\n",
    "def add_likelihood_feature(fname, train_likeli, test_likeli, flist):\n",
    "    tt_likeli = pd.DataFrame()\n",
    "    np.random.seed(1232345)\n",
    "    #Chia fold để tính toán các giá trị\n",
    "    #Tập test được điền theo tập train\n",
    "    skf = StratifiedKFold(train_likeli['TARGET'].values, n_folds=5, shuffle=True, random_state=21387)\n",
    "    for train_index, test_index in skf:\n",
    "        ids = train_likeli['ID'].values[train_index]\n",
    "        train_fold = train_likeli.loc[train_likeli['ID'].isin(ids)].copy()\n",
    "        test_fold = train_likeli.loc[~train_likeli['ID'].isin(ids)].copy()\n",
    "        global_avg = np.mean(train_fold['TARGET'].values)\n",
    "        feats_likeli = train_fold.groupby(fname)['TARGET'].agg({'sum': np.sum, 'count': len}).reset_index()\n",
    "        feats_likeli[fname + '_likeli'] = (feats_likeli['sum'] + 30.0*global_avg)/(feats_likeli['count']+30.0)\n",
    "        test_fold = pd.merge(test_fold, feats_likeli[[fname, fname + '_likeli']], on=fname, how='left')\n",
    "        test_fold[fname + '_likeli'] = test_fold[fname + '_likeli'].fillna(global_avg)\n",
    "        tt_likeli = tt_likeli.append(test_fold[['ID', fname + '_likeli']], ignore_index=True)\n",
    "    train_likeli = pd.merge(train_likeli, tt_likeli, on='ID', how='left')\n",
    "    \n",
    "    global_avg = np.mean(train_likeli['TARGET'].values)\n",
    "    feats_likeli = train_likeli.groupby(fname)['TARGET'].agg({'sum': np.sum, 'count': len}).reset_index()\n",
    "    feats_likeli[fname + '_likeli'] = (feats_likeli['sum'] + 30.0*global_avg)/(feats_likeli['count']+30.0)\n",
    "    test_likeli = pd.merge(test_likeli, feats_likeli[[fname, fname + '_likeli']], on=fname, how='left')\n",
    "    test_likeli[fname + '_likeli'] = test_likeli[fname + '_likeli'].fillna(global_avg)\n",
    "    return train_likeli, test_likeli, flist + [fname + '_likeli']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7b552cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_features(train, test, features):\n",
    "    flist = [x for x in train.columns if not x in ['ID','TARGET']]\n",
    "    if 'SumZeros' in features:\n",
    "        train.insert(1, 'SumZeros', (train[flist] == 0).astype(int).sum(axis=1))\n",
    "        test.insert(1, 'SumZeros', (test[flist] == 0).astype(int).sum(axis=1))\n",
    "    flist = [x for x in train.columns if not x in ['ID','TARGET']]\n",
    "\n",
    "    if 'tsne' in features:\n",
    "        tsne_feats = pd.read_csv(OUTPUT_PATH + 'features/tsne_feats.csv')\n",
    "        train = pd.merge(train, tsne_feats, on='ID', how='left')\n",
    "        test = pd.merge(test, tsne_feats, on='ID', how='left')\n",
    "\n",
    "    if 'pca' in features:\n",
    "        pca_feats = pd.read_csv(OUTPUT_PATH + 'features/dmitry_pca_feats.csv')\n",
    "        train = pd.merge(train, pca_feats, on='ID', how='left')\n",
    "        test = pd.merge(test, pca_feats, on='ID', how='left')\n",
    "\n",
    "    if 'kmeans' in features:\n",
    "        kmeans_feats = pd.read_csv(OUTPUT_PATH + 'features/kmeans_feats.csv')\n",
    "        train = pd.merge(train, kmeans_feats, on='ID', how='left')\n",
    "        test = pd.merge(test, kmeans_feats, on='ID', how='left')\n",
    "\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3fbdea",
   "metadata": {},
   "source": [
    "#### Mô hình"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f470e5d4",
   "metadata": {},
   "source": [
    "#####  FTRL2 (Follow the Regularized Leader)\n",
    "Sử dụng dữ liệu gốc, Sumzeros và Likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8866e1dc",
   "metadata": {},
   "source": [
    "#####  Regularized Greedy Forest 3\n",
    "Sử dụng dữ liệu gốc, Sumzeros, PCA và Likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6fd685a",
   "metadata": {},
   "source": [
    "#####  Regularized Greedy Forest 5\n",
    "Sử dụng dữ liệu gốc, Sumzeros, tSNE và Likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14548000",
   "metadata": {},
   "source": [
    "#####  Regularized Greedy Forest 6\n",
    "Sử dụng dữ liệu gốc, Sumzeros, K-means và Likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4c4c50",
   "metadata": {},
   "source": [
    "#####  Adaboost classifier\n",
    "Sử dụng dữ liệu gốc, Sumzeros, PCA và Likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d19bd6",
   "metadata": {},
   "source": [
    "#####  XGBOOST\n",
    "Sử dụng dữ liệu gốc, Sumzeros, PCA và Likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1769f4",
   "metadata": {},
   "source": [
    "### Nhóm mô hình Ikki Tanaka"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95e8d77",
   "metadata": {},
   "source": [
    "#### Tiền xử lý dữ liệu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33c41eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_train = pd.read_csv('train.csv')\n",
    "ori_test = pd.read_csv('test.csv')\n",
    "sample_submit = pd.read_csv('sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f08515b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_train['target'] = ori_train['TARGET']\n",
    "ori_train['t_id'] = ori_train[\"ID\"]\n",
    "ori_test['t_id'] = ori_test[\"ID\"]\n",
    "\n",
    "del ori_train['TARGET'], ori_train[\"ID\"], ori_test[\"ID\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83029a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_rfe(ori_train, ori_test):\n",
    "    print('cleaning...')\n",
    "    # bad features for delete such as A = {1,2,3} * B, \n",
    "    bad = ['num_var6_0', 'num_var6', 'num_var8', 'num_var13_medio_0', 'num_var13_medio', 'num_var18_0', 'num_var18', 'num_var20_0', 'num_var20', 'num_var29_0', 'num_var29', 'num_var34_0', 'num_var34', 'num_var44', 'delta_imp_amort_var18_1y3', 'delta_imp_amort_var34_1y3', 'num_var7_emit_ult1', 'num_meses_var13_medio_ult3']\n",
    "\n",
    "    for i in bad:\n",
    "        del ori_train[i], ori_test[i]\n",
    "    assert( all(ori_train.columns == ori_test.columns))\n",
    "\n",
    "    \n",
    "    # Dataframe for saving\n",
    "    train_df = pd.DataFrame()\n",
    "    test_df = pd.DataFrame()\n",
    "\n",
    "\n",
    "    # temporal information?\n",
    "    # (ori_train[['num_var45_hace3','num_var45_hace2','num_var45_ult1']] + 1).T.pct_change().T\n",
    "    tinfo = [\n",
    "            ['num_op_var40_hace3','num_op_var40_hace2','num_op_var40_ult1','num_op_var40_ult3'],\n",
    "            ['num_op_var41_hace3','num_op_var41_hace2','num_op_var41_ult1','num_op_var41_ult3'],\n",
    "            ['num_var22_hace3', 'num_var22_hace2', 'num_var22_ult1', 'num_var22_ult3'],\n",
    "            ['num_var45_hace3', 'num_var45_hace2', 'num_var45_ult1', 'num_var45_ult3'],\n",
    "            ]\n",
    "\n",
    "    #pct_change and sub\n",
    "    for i in tinfo:\n",
    "        a_tr = (ori_train[i] + 0.01).T.pct_change().T.iloc[:,1:]\n",
    "        a_tr.columns += '_pct_change'\n",
    "        a_te = (ori_test[i] + 0.01).T.pct_change().T.iloc[:,1:]\n",
    "        a_te.columns += '_pct_change'\n",
    "        train_df = pd.concat([train_df, a_tr], axis=1)\n",
    "        test_df = pd.concat([test_df, a_te], axis=1)\n",
    "        #sub\n",
    "        train_df['{}-{}'.format(i[3],i[2])] = ori_train[i[3]] - ori_train[i[2]]\n",
    "        train_df['{}-{}'.format(i[3],i[1])] = ori_train[i[3]] - ori_train[i[1]]\n",
    "        train_df['{}-{}'.format(i[3],i[0])] = ori_train[i[3]] - ori_train[i[0]]\n",
    "        test_df['{}-{}'.format(i[3],i[2])] = ori_test[i[3]] - ori_test[i[2]]\n",
    "        test_df['{}-{}'.format(i[3],i[1])] = ori_test[i[3]] - ori_test[i[1]]\n",
    "        test_df['{}-{}'.format(i[3],i[0])] = ori_test[i[3]] - ori_test[i[0]]\n",
    "    assert( all(train_df.columns == test_df.columns))\n",
    "\n",
    "\n",
    "\n",
    "    # comer\n",
    "    comer = [\n",
    "            ['imp_op_var39_comer_ult1','imp_op_var40_comer_ult1','imp_op_var41_comer_ult1'],\n",
    "            ['imp_op_var39_comer_ult3','imp_op_var40_comer_ult3','imp_op_var41_comer_ult3'],\n",
    "            ['num_op_var39_comer_ult1','num_op_var40_comer_ult1','num_op_var41_comer_ult1'],\n",
    "            ['num_op_var39_comer_ult3','num_op_var40_comer_ult3','num_op_var41_comer_ult3'],\n",
    "            ]\n",
    "\n",
    "    def percent_row(data):\n",
    "        #print data\n",
    "        lhs = data.index[0]\n",
    "        sum_ = data[1:].sum()\n",
    "        data.index += '_pct_imp_op_' + lhs\n",
    "        return pd.Series(data[1:])/(sum_+0.1)\n",
    "\n",
    "    for i in xrange(len(comer)):\n",
    "        train_df = pd.concat([train_df, ori_train[comer[i]].apply(percent_row, axis=1)],axis=1)\n",
    "        test_df = pd.concat([test_df, ori_test[comer[i]].apply(percent_row, axis=1)],axis=1)\n",
    "    assert( all(train_df.columns == test_df.columns))\n",
    "\n",
    "\n",
    "\n",
    "    # efect\n",
    "    efect = [\n",
    "            ['imp_op_var39_efect_ult1','imp_op_var40_efect_ult1','imp_op_var41_efect_ult1'],\n",
    "            ['imp_op_var39_efect_ult3','imp_op_var40_efect_ult3','imp_op_var41_efect_ult3'],\n",
    "            ['num_op_var39_efect_ult1','num_op_var40_efect_ult1','num_op_var41_efect_ult1'],\n",
    "            ['num_op_var39_efect_ult3','num_op_var40_efect_ult3','num_op_var41_efect_ult3'],\n",
    "            ]\n",
    "\n",
    "    def percent_row(data):\n",
    "        #print data\n",
    "        lhs = data.index[0]\n",
    "        sum_ = data[1:].sum()\n",
    "        data.index += '_pct_imp_op_' + lhs\n",
    "        return pd.Series(data[1:])/(sum_+0.1)\n",
    "\n",
    "    for i in xrange(len(efect)):\n",
    "        train_df = pd.concat([train_df, ori_train[efect[i]].apply(percent_row, axis=1)],axis=1)\n",
    "        test_df = pd.concat([test_df, ori_test[efect[i]].apply(percent_row, axis=1)],axis=1)\n",
    "    assert( all(train_df.columns == test_df.columns))\n",
    "\n",
    "\n",
    "\n",
    "    # imp_op\n",
    "    imp_op = [['imp_op_var39_ult1','imp_op_var40_ult1','imp_op_var41_ult1']]\n",
    "\n",
    "    def percent_row(data):\n",
    "        #print data\n",
    "        lhs = data.index[0]\n",
    "        sum_ = data[1:].sum()\n",
    "        data.index += '_pct_imp_op_' + lhs\n",
    "        return pd.Series(data[1:])/(sum_+0.1)\n",
    "\n",
    "    for i in xrange(len(imp_op)):\n",
    "        train_df = pd.concat([train_df, ori_train[imp_op[i]].apply(percent_row, axis=1)],axis=1)\n",
    "        test_df = pd.concat([test_df, ori_test[imp_op[i]].apply(percent_row, axis=1)],axis=1)\n",
    "    assert( all(train_df.columns == test_df.columns))\n",
    "\n",
    "\n",
    "\n",
    "    # num_op\n",
    "    num_op = [\n",
    "            ['num_op_var39_hace2','num_op_var40_hace2','num_op_var41_hace2'],\n",
    "            ['num_op_var39_hace3','num_op_var40_hace3','num_op_var41_hace3'],\n",
    "            ['num_op_var39_ult1','num_op_var40_ult1','num_op_var41_ult1'],\n",
    "            ['num_op_var39_ult3','num_op_var40_ult1','num_op_var40_hace2','num_op_var40_hace3','num_op_var41_ult1','num_op_var41_hace2','num_op_var41_hace3'],\n",
    "            ]\n",
    "\n",
    "    def percent_row(data):\n",
    "        #print data\n",
    "        lhs = data.index[0]\n",
    "        sum_ = data[1:].sum()\n",
    "        data.index += '_pct_num_op_' + lhs\n",
    "        return pd.Series(data[1:])/(sum_+0.1)\n",
    "\n",
    "    for i in xrange(len(num_op)):\n",
    "        train_df = pd.concat([train_df, ori_train[num_op[i]].apply(percent_row, axis=1)], axis=1)\n",
    "        test_df = pd.concat([test_df, ori_test[num_op[i]].apply(percent_row, axis=1)], axis=1)\n",
    "    assert( all(train_df.columns == test_df.columns))\n",
    "\n",
    "\n",
    "    #make features of var39, var40, and var41\n",
    "    varl = ['var39', 'var40', 'var41']\n",
    "    for i in varl:\n",
    "        #train\n",
    "        #imp_op_var39_ult1\n",
    "        train_df['imp_op_{}_efect_ult1/imp_op_{}_ult1'.format(i,i)] = ori_train['imp_op_{}_efect_ult1'.format(i)] / (ori_train['imp_op_{}_ult1'.format(i)] + 0.01)\n",
    "        train_df['imp_op_{}_comer_ult1/imp_op_{}_ult1'.format(i,i)] = ori_train['imp_op_{}_comer_ult1'.format(i)] / (ori_train['imp_op_{}_ult1'.format(i)] + 0.01)\n",
    "        train_df['imp_op_{}_efect_ult1/imp_op_{}_comer_ult1'.format(i,i)] = ori_train['imp_op_{}_efect_ult1'.format(i)] / (ori_train['imp_op_{}_comer_ult1'.format(i)] + 0.01)\n",
    "\n",
    "        #train_df['imp_op_{}_efect_ult3/imp_op_{}_ult3'.format(i,i)] = ori_train['imp_op_{}_efect_ult3'.format(i)] / (ori_train['imp_op_{}_ult3'.format(i)] + 0.01)\n",
    "        #train_df['imp_op_{}_comer_ult3/imp_op_{}_ult3'.format(i,i)] = ori_train['imp_op_{}_comer_ult3'.format(i)] / (ori_train['imp_op_{}_ult3'.format(i)] + 0.01)\n",
    "        train_df['imp_op_{}_efect_ult3/imp_op_{}_comer_ult3'.format(i,i)] = ori_train['imp_op_{}_efect_ult3'.format(i)] / (ori_train['imp_op_{}_comer_ult3'.format(i)] + 0.01)\n",
    "\n",
    "        #num_op_var39_ult1\n",
    "        train_df['num_op_{}_efect_ult1/num_op_{}_ult1'.format(i,i)] = ori_train['num_op_{}_efect_ult1'.format(i)] / (ori_train['num_op_{}_ult1'.format(i)] + 0.01)\n",
    "        train_df['num_op_{}_comer_ult1/num_op_{}_ult1'.format(i,i)] = ori_train['num_op_{}_comer_ult1'.format(i)] / (ori_train['num_op_{}_ult1'.format(i)] + 0.01)\n",
    "        train_df['num_op_{}_efect_ult1/num_op_{}_comer_ult1'.format(i,i)] = ori_train['num_op_{}_efect_ult1'.format(i)] / (ori_train['num_op_{}_comer_ult1'.format(i)] + 0.01)\n",
    "\n",
    "        train_df['num_op_{}_efect_ult3/num_op_{}_ult3'.format(i,i)] = ori_train['num_op_{}_efect_ult3'.format(i)] / (ori_train['num_op_{}_ult3'.format(i)] + 0.01)\n",
    "        train_df['num_op_{}_comer_ult3/num_op_{}_ult3'.format(i,i)] = ori_train['num_op_{}_comer_ult3'.format(i)] / (ori_train['num_op_{}_ult3'.format(i)] + 0.01)\n",
    "        train_df['num_op_{}_efect_ult3/num_op_{}_comer_ult3'.format(i,i)] = ori_train['num_op_{}_efect_ult3'.format(i)] / (ori_train['num_op_{}_comer_ult3'.format(i)] + 0.01)\n",
    "\n",
    "        #num/imp_op_var39_ult1\n",
    "        train_df['imp_op_{}_efect_ult1/num_op_{}_ult1'.format(i,i)] = ori_train['imp_op_{}_efect_ult1'.format(i)] / (ori_train['num_op_{}_ult1'.format(i)] + 0.01)\n",
    "        train_df['imp_op_{}_comer_ult1/num_op_{}_ult1'.format(i,i)] = ori_train['imp_op_{}_comer_ult1'.format(i)] / (ori_train['num_op_{}_ult1'.format(i)] + 0.01)\n",
    "        train_df['imp_op_{}_efect_ult1/num_op_{}_comer_ult1'.format(i,i)] = ori_train['imp_op_{}_efect_ult1'.format(i)] / (ori_train['num_op_{}_comer_ult1'.format(i)] + 0.01)\n",
    "\n",
    "        train_df['imp_op_{}_efect_ult3/num_op_{}_ult3'.format(i,i)] = ori_train['imp_op_{}_efect_ult3'.format(i)] / (ori_train['num_op_{}_ult3'.format(i)] + 0.01)\n",
    "        train_df['imp_op_{}_comer_ult3/num_op_{}_ult3'.format(i,i)] = ori_train['imp_op_{}_comer_ult3'.format(i)] / (ori_train['num_op_{}_ult3'.format(i)] + 0.01)\n",
    "        train_df['imp_op_{}_efect_ult3/num_op_{}_comer_ult3'.format(i,i)] = ori_train['imp_op_{}_efect_ult3'.format(i)] / (ori_train['num_op_{}_comer_ult3'.format(i)] + 0.01)\n",
    "\n",
    "        #test\n",
    "        #imp_op_var39_ult1\n",
    "        test_df['imp_op_{}_efect_ult1/imp_op_{}_ult1'.format(i,i)] = ori_test['imp_op_{}_efect_ult1'.format(i)] / (ori_test['imp_op_{}_ult1'.format(i)] + 0.01)\n",
    "        test_df['imp_op_{}_comer_ult1/imp_op_{}_ult1'.format(i,i)] = ori_test['imp_op_{}_comer_ult1'.format(i)] / (ori_test['imp_op_{}_ult1'.format(i)] + 0.01)\n",
    "        test_df['imp_op_{}_efect_ult1/imp_op_{}_comer_ult1'.format(i,i)] = ori_test['imp_op_{}_efect_ult1'.format(i)] / (ori_test['imp_op_{}_comer_ult1'.format(i)] + 0.01)\n",
    "\n",
    "        #test_df['imp_op_{}_efect_ult3/imp_op_{}_ult3'.format(i,i)] = ori_test['imp_op_{}_efect_ult3'.format(i)] / (ori_test['imp_op_{}_ult3'.format(i)] + 0.01)\n",
    "        #test_df['imp_op_{}_comer_ult3/imp_op_{}_ult3'.format(i,i)] = ori_test['imp_op_{}_comer_ult3'.format(i)] / (ori_test['imp_op_{}_ult3'.format(i)] + 0.01)\n",
    "        test_df['imp_op_{}_efect_ult3/imp_op_{}_comer_ult3'.format(i,i)] = ori_test['imp_op_{}_efect_ult3'.format(i)] / (ori_test['imp_op_{}_comer_ult3'.format(i)] + 0.01)\n",
    "\n",
    "        #num_op_var39_ult1\n",
    "        test_df['num_op_{}_efect_ult1/num_op_{}_ult1'.format(i,i)] = ori_test['num_op_{}_efect_ult1'.format(i)] / (ori_test['num_op_{}_ult1'.format(i)] + 0.01)\n",
    "        test_df['num_op_{}_comer_ult1/num_op_{}_ult1'.format(i,i)] = ori_test['num_op_{}_comer_ult1'.format(i)] / (ori_test['num_op_{}_ult1'.format(i)] + 0.01)\n",
    "        test_df['num_op_{}_efect_ult1/num_op_{}_comer_ult1'.format(i,i)] = ori_test['num_op_{}_efect_ult1'.format(i)] / (ori_test['num_op_{}_comer_ult1'.format(i)] + 0.01)\n",
    "\n",
    "        test_df['num_op_{}_efect_ult3/num_op_{}_ult3'.format(i,i)] = ori_test['num_op_{}_efect_ult3'.format(i)] / (ori_test['num_op_{}_ult3'.format(i)] + 0.01)\n",
    "        test_df['num_op_{}_comer_ult3/num_op_{}_ult3'.format(i,i)] = ori_test['num_op_{}_comer_ult3'.format(i)] / (ori_test['num_op_{}_ult3'.format(i)] + 0.01)\n",
    "        test_df['num_op_{}_efect_ult3/num_op_{}_comer_ult3'.format(i,i)] = ori_test['num_op_{}_efect_ult3'.format(i)] / (ori_test['num_op_{}_comer_ult3'.format(i)] + 0.01)\n",
    "\n",
    "        #num/imp_op_var39_ult1\n",
    "        test_df['imp_op_{}_efect_ult1/num_op_{}_ult1'.format(i,i)] = ori_train['imp_op_{}_efect_ult1'.format(i)] / (ori_train['num_op_{}_ult1'.format(i)] + 0.01)\n",
    "        test_df['imp_op_{}_comer_ult1/num_op_{}_ult1'.format(i,i)] = ori_train['imp_op_{}_comer_ult1'.format(i)] / (ori_train['num_op_{}_ult1'.format(i)] + 0.01)\n",
    "        test_df['imp_op_{}_efect_ult1/num_op_{}_comer_ult1'.format(i,i)] = ori_train['imp_op_{}_efect_ult1'.format(i)] / (ori_train['num_op_{}_comer_ult1'.format(i)] + 0.01)\n",
    "\n",
    "        test_df['imp_op_{}_efect_ult3/num_op_{}_ult3'.format(i,i)] = ori_train['imp_op_{}_efect_ult3'.format(i)] / (ori_train['num_op_{}_ult3'.format(i)] + 0.01)\n",
    "        test_df['imp_op_{}_comer_ult3/num_op_{}_ult3'.format(i,i)] = ori_train['imp_op_{}_comer_ult3'.format(i)] / (ori_train['num_op_{}_ult3'.format(i)] + 0.01)\n",
    "        test_df['imp_op_{}_efect_ult3/num_op_{}_comer_ult3'.format(i,i)] = ori_train['imp_op_{}_efect_ult3'.format(i)] / (ori_train['num_op_{}_comer_ult3'.format(i)] + 0.01)\n",
    "\n",
    "\n",
    "    assert( all(train_df.columns == test_df.columns))\n",
    "\n",
    "\n",
    "\n",
    "    # saldo_var13 = 1 * saldo_var13_corto + 1 * saldo_var13_largo + 1 * saldo_var13_medio #\n",
    "    saldo = [\n",
    "            ['saldo_var13','saldo_var13_corto','saldo_var13_medio','saldo_var13_largo'],\n",
    "            ]\n",
    "\n",
    "    def percent_row(data):\n",
    "        #print data\n",
    "        lhs = data.index[0]\n",
    "        sum_ = data.sum()\n",
    "        data.index += '_pct_saldo_' + lhs \n",
    "        return pd.Series(data)/(sum_+0.1)\n",
    "\n",
    "    train_df = pd.concat([train_df, ori_train[saldo[0]].apply(percent_row, axis=1)],axis=1)\n",
    "    test_df = pd.concat([test_df, ori_test[saldo[0]].apply(percent_row, axis=1)],axis=1)\n",
    "    assert( all(train_df.columns == test_df.columns))\n",
    "\n",
    "\n",
    "\n",
    "    #num_var\n",
    "    num_var = [\n",
    "            ['num_var13_0','num_var13_corto_0','ind_var13_medio_0','num_var13_largo_0'],\n",
    "            ['num_var13','num_var13_corto','ind_var13_medio','num_var13_largo'],\n",
    "            ]\n",
    "\n",
    "    def percent_row(data):\n",
    "        #print data\n",
    "        lhs = data.index[0]\n",
    "        sum_ = data.sum()\n",
    "        data.index += '_pct_num_var' + lhs\n",
    "        return pd.Series(data)/(sum_+0.1)\n",
    "\n",
    "    for i in xrange(len(num_var)):\n",
    "        train_df = pd.concat([train_df, ori_train[num_var[i]].apply(percent_row, axis=1)],axis=1)\n",
    "        test_df = pd.concat([test_df, ori_test[num_var[i]].apply(percent_row, axis=1)],axis=1)\n",
    "\n",
    "    assert( all(train_df.columns == test_df.columns))\n",
    "\n",
    "    ori_train = pd.concat([ori_train, train_df], axis=1)\n",
    "    ori_test = pd.concat([ori_test, test_df], axis=1)\n",
    "    \n",
    "\n",
    "    return ori_train, ori_test\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b91d9212",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_feat(train, test, sample_submit=None):\n",
    "    \n",
    "    train_target = train['target']\n",
    "    del train['target']\n",
    "\n",
    "    #delete id\n",
    "    del train['t_id'], test['t_id']\n",
    "\n",
    "    # 0 count per ID\n",
    "    def countZero(data):\n",
    "        return np.sum(data == 0)\n",
    "\n",
    "    train['count0'] = train.apply(countZero, axis=1)\n",
    "    test['count0'] = test.apply(countZero, axis=1)\n",
    "\n",
    "    # add count features of integer columns\n",
    "    int_col = (train.dtypes == np.int64)[(train.dtypes == np.int64).values].index\n",
    "    train_test = pd.concat([train,test])\n",
    "    for i in int_col:\n",
    "        tmp_cnt = train_test[i].value_counts()\n",
    "        tmp_cnt = tmp_cnt.to_frame(name=i+'_cnt')\n",
    "        tmp_cnt[i] = tmp_cnt.index\n",
    "        tmp_cnt.reset_index(drop=True, inplace=True)\n",
    "        train = train.reset_index().merge(tmp_cnt, how='left', on=i).sort('index').drop('index', axis=1)\n",
    "        test = test.reset_index().merge(tmp_cnt, how='left', on=i).sort('index').drop('index', axis=1)\n",
    "        train.reset_index(drop=True, inplace=True)\n",
    "        test.reset_index(drop=True, inplace=True)\n",
    "    del train_test\n",
    "\n",
    "    ###### cleaing data using reverse feature engineering ######\n",
    "    # To enable cleaning with reverse feature engineering,     #\n",
    "    # comment out the next lines                               #\n",
    "    ############################################################\n",
    "\n",
    "    #print 'starting cleaning_rfe...'\n",
    "    #train, test = cleaning_rfe(ori_train=train.copy(), ori_test=test.copy())\n",
    "    #print 'done cleaning_rfe'\n",
    "\n",
    "    # make dummy variables of var3 in the threshold(>=5)\n",
    "    var3_cnt = train.var3.value_counts()\n",
    "    #threshold is different from feat_ver407(var3_cnt>=5)\n",
    "    index_var3_th = var3_cnt[(var3_cnt>=4).values].index\n",
    "    train['var3_tmp'] = train.var3.apply(lambda x: x if x in index_var3_th else np.nan)\n",
    "    test['var3_tmp'] = test.var3.apply(lambda x: x if x in index_var3_th else np.nan)\n",
    "    \n",
    "    train_test = pd.concat([train,test])\n",
    "    #train_test.reset_index(drop=True, inplace=True)\n",
    "    tmp = pd.get_dummies(train_test['var3_tmp'], prefix='ohe_var3', prefix_sep='_')\n",
    "\n",
    "    train = pd.concat([train, tmp.iloc[:len(train),:]], axis=1)\n",
    "    test = pd.concat([test, tmp.iloc[len(train):,:]], axis=1)\n",
    "    del train['var3_tmp'], test['var3_tmp']\n",
    "\n",
    "    # add feature of var38\n",
    "    train['var38mc'] = np.isclose(train.var38, 117310.979016)\n",
    "    train['logvar38'] = train.loc[~train['var38mc'], 'var38'].map(np.log)\n",
    "    train.loc[train['var38mc'], 'logvar38'] = 0\n",
    "\n",
    "    test['var38mc'] = np.isclose(test.var38, 117310.979016)\n",
    "    test['logvar38'] = test.loc[~test['var38mc'], 'var38'].map(np.log)\n",
    "    test.loc[test['var38mc'], 'logvar38'] = 0\n",
    "\n",
    "    train['var38mc'] = train['var38mc'].astype(int)\n",
    "\n",
    "    test['var38mc'] = test['var38mc'].astype(int)\n",
    "\n",
    "    #delete constant features\n",
    "    for i in train.columns:\n",
    "        if len(set(train[i].values)) == 1:\n",
    "            del train[i], test[i]\n",
    "    assert( all(train.columns == test.columns))\n",
    "\n",
    "    #delete identical columns\n",
    "    unique_col = train.T.drop_duplicates().T.columns\n",
    "    train = train[unique_col]\n",
    "    test = test[unique_col]\n",
    "    assert( all(train.columns == test.columns))\n",
    "\n",
    "\n",
    "    train['target'] = train_target\n",
    "\n",
    "    train.to_csv('features_train_ver2.csv',index=None)\n",
    "    test.to_csv('features_test_ver2.csv',index=None)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "09730c76",
   "metadata": {},
   "outputs": [],
   "source": [
    " main_feat(train=ori_train.copy(), test=ori_test.copy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a57c72",
   "metadata": {},
   "source": [
    "#### Mô hình"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c219578",
   "metadata": {},
   "source": [
    "### Nhóm mô hình Darius Barusauskas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2afdd2",
   "metadata": {},
   "source": [
    "#### Tiền xử lý dữ liệu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbbab5ab",
   "metadata": {},
   "source": [
    "#### Mô hình"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17f2ee0",
   "metadata": {},
   "source": [
    "### Nhóm mô hình Marios Michailidis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3cff8a",
   "metadata": {},
   "source": [
    "#### Tiền xử lý dữ liệu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3461999",
   "metadata": {},
   "source": [
    "* Sử dụng lại một số kết quả tiền xử lý dữ liệu của các mô hình trước. Bao gồm loại bỏ 63 cột chỉ có một giá trị và liên quan đến các cột khác, nên không cung cấp được nhiều thông tin. Một cột được tạo mới nhằm đếm số lượng số 0 trong mỗi hàng.\n",
    "* Một số bước tiền xử lý khác nhau được tiến hành trong các mô hình, bao gồm:\n",
    "    1. Standard scaling đối với các mô hình tuyến tính và Neural Networks\n",
    "    2. Đếm từng biến mỗi loại\n",
    "    3. Tính xác suất của các biến\n",
    "    4. Phân tích t-SNE, PCA và kNN\n",
    "    5. Kết hợp các phương pháp trên"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62c850d",
   "metadata": {},
   "source": [
    "#### Mô hình"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144345e3",
   "metadata": {},
   "source": [
    "### Nhóm mô hình Mathias Muller"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca515cf",
   "metadata": {},
   "source": [
    "#### Tiền xử lý dữ liệu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3eeb2e4",
   "metadata": {},
   "source": [
    "#### Mô hình"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c52174",
   "metadata": {},
   "source": [
    "## Tài liệu tham khảo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5823df",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
