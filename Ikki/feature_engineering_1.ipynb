{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 899,
     "status": "ok",
     "timestamp": 1639199191590,
     "user": {
      "displayName": "Huy Lê",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "04255824725604749089"
     },
     "user_tz": -420
    },
    "id": "RAW6X5_L3Ul1"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from datetime import date\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import Normalizer, PolynomialFeatures, MinMaxScaler\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 8268,
     "status": "ok",
     "timestamp": 1639199202522,
     "user": {
      "displayName": "Huy Lê",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "04255824725604749089"
     },
     "user_tz": -420
    },
    "id": "P7x_HSzW3tQC"
   },
   "outputs": [],
   "source": [
    "ori_train = pd.read_csv('train.csv')\n",
    "ori_test = pd.read_csv('test.csv')\n",
    "sample_submit = pd.read_csv('sample_submission.csv')\n",
    "\n",
    "ori_train['target'] = ori_train['TARGET']\n",
    "ori_train['t_id'] = ori_train[\"ID\"]\n",
    "ori_test['t_id'] = ori_test[\"ID\"]\n",
    "\n",
    "del ori_train['TARGET'], ori_train[\"ID\"], ori_test[\"ID\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 779,
     "status": "ok",
     "timestamp": 1639199230311,
     "user": {
      "displayName": "Huy Lê",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "04255824725604749089"
     },
     "user_tz": -420
    },
    "id": "H1Uw5U193wKa"
   },
   "outputs": [],
   "source": [
    "def cleaning_rfe(ori_train, ori_test):\n",
    "    print ('cleaning...')\n",
    "    # bad features for delete such as A = {1,2,3} * B, \n",
    "    bad = ['num_var6_0', 'num_var6', 'num_var8', 'num_var13_medio_0', 'num_var13_medio', 'num_var18_0', 'num_var18', 'num_var20_0', 'num_var20', 'num_var29_0', 'num_var29', 'num_var34_0', 'num_var34', 'num_var44', 'delta_imp_amort_var18_1y3', 'delta_imp_amort_var34_1y3', 'num_var7_emit_ult1', 'num_meses_var13_medio_ult3']\n",
    "\n",
    "    for i in bad:\n",
    "        del ori_train[i], ori_test[i]\n",
    "    assert( all(ori_train.columns == ori_test.columns))\n",
    "\n",
    "    \n",
    "    # Dataframe for saving\n",
    "    train_df = pd.DataFrame()\n",
    "    test_df = pd.DataFrame()\n",
    "\n",
    "\n",
    "    # temporal information?\n",
    "    # (ori_train[['num_var45_hace3','num_var45_hace2','num_var45_ult1']] + 1).T.pct_change().T\n",
    "    tinfo = [\n",
    "            ['num_op_var40_hace3','num_op_var40_hace2','num_op_var40_ult1','num_op_var40_ult3'],\n",
    "            ['num_op_var41_hace3','num_op_var41_hace2','num_op_var41_ult1','num_op_var41_ult3'],\n",
    "            ['num_var22_hace3', 'num_var22_hace2', 'num_var22_ult1', 'num_var22_ult3'],\n",
    "            ['num_var45_hace3', 'num_var45_hace2', 'num_var45_ult1', 'num_var45_ult3'],\n",
    "            ]\n",
    "\n",
    "    #pct_change and sub\n",
    "    for i in tinfo:\n",
    "        a_tr = (ori_train[i] + 0.01).T.pct_change().T.iloc[:,1:]\n",
    "        a_tr.columns += '_pct_change'\n",
    "        a_te = (ori_test[i] + 0.01).T.pct_change().T.iloc[:,1:]\n",
    "        a_te.columns += '_pct_change'\n",
    "        train_df = pd.concat([train_df, a_tr], axis=1)\n",
    "        test_df = pd.concat([test_df, a_te], axis=1)\n",
    "        #sub\n",
    "        train_df['{}-{}'.format(i[3],i[2])] = ori_train[i[3]] - ori_train[i[2]]\n",
    "        train_df['{}-{}'.format(i[3],i[1])] = ori_train[i[3]] - ori_train[i[1]]\n",
    "        train_df['{}-{}'.format(i[3],i[0])] = ori_train[i[3]] - ori_train[i[0]]\n",
    "        test_df['{}-{}'.format(i[3],i[2])] = ori_test[i[3]] - ori_test[i[2]]\n",
    "        test_df['{}-{}'.format(i[3],i[1])] = ori_test[i[3]] - ori_test[i[1]]\n",
    "        test_df['{}-{}'.format(i[3],i[0])] = ori_test[i[3]] - ori_test[i[0]]\n",
    "    assert( all(train_df.columns == test_df.columns))\n",
    "\n",
    "\n",
    "\n",
    "    # comer\n",
    "    comer = [\n",
    "            ['imp_op_var39_comer_ult1','imp_op_var40_comer_ult1','imp_op_var41_comer_ult1'],\n",
    "            ['imp_op_var39_comer_ult3','imp_op_var40_comer_ult3','imp_op_var41_comer_ult3'],\n",
    "            ['num_op_var39_comer_ult1','num_op_var40_comer_ult1','num_op_var41_comer_ult1'],\n",
    "            ['num_op_var39_comer_ult3','num_op_var40_comer_ult3','num_op_var41_comer_ult3'],\n",
    "            ]\n",
    "\n",
    "    def percent_row(data):\n",
    "        #print data\n",
    "        lhs = data.index[0]\n",
    "        sum_ = data[1:].sum()\n",
    "        data.index += '_pct_imp_op_' + lhs\n",
    "        return pd.Series(data[1:])/(sum_+0.1)\n",
    "\n",
    "    for i in range(len(comer)):\n",
    "        train_df = pd.concat([train_df, ori_train[comer[i]].apply(percent_row, axis=1)],axis=1)\n",
    "        test_df = pd.concat([test_df, ori_test[comer[i]].apply(percent_row, axis=1)],axis=1)\n",
    "    assert( all(train_df.columns == test_df.columns))\n",
    "\n",
    "\n",
    "\n",
    "    # efect\n",
    "    efect = [\n",
    "            ['imp_op_var39_efect_ult1','imp_op_var40_efect_ult1','imp_op_var41_efect_ult1'],\n",
    "            ['imp_op_var39_efect_ult3','imp_op_var40_efect_ult3','imp_op_var41_efect_ult3'],\n",
    "            ['num_op_var39_efect_ult1','num_op_var40_efect_ult1','num_op_var41_efect_ult1'],\n",
    "            ['num_op_var39_efect_ult3','num_op_var40_efect_ult3','num_op_var41_efect_ult3'],\n",
    "            ]\n",
    "\n",
    "    def percent_row(data):\n",
    "        #print data\n",
    "        lhs = data.index[0]\n",
    "        sum_ = data[1:].sum()\n",
    "        data.index += '_pct_imp_op_' + lhs\n",
    "        return pd.Series(data[1:])/(sum_+0.1)\n",
    "\n",
    "    for i in range(len(efect)):\n",
    "        train_df = pd.concat([train_df, ori_train[efect[i]].apply(percent_row, axis=1)],axis=1)\n",
    "        test_df = pd.concat([test_df, ori_test[efect[i]].apply(percent_row, axis=1)],axis=1)\n",
    "    assert( all(train_df.columns == test_df.columns))\n",
    "\n",
    "\n",
    "\n",
    "    # imp_op\n",
    "    imp_op = [['imp_op_var39_ult1','imp_op_var40_ult1','imp_op_var41_ult1']]\n",
    "\n",
    "    def percent_row(data):\n",
    "        #print data\n",
    "        lhs = data.index[0]\n",
    "        sum_ = data[1:].sum()\n",
    "        data.index += '_pct_imp_op_' + lhs\n",
    "        return pd.Series(data[1:])/(sum_+0.1)\n",
    "\n",
    "    for i in range(len(imp_op)):\n",
    "        train_df = pd.concat([train_df, ori_train[imp_op[i]].apply(percent_row, axis=1)],axis=1)\n",
    "        test_df = pd.concat([test_df, ori_test[imp_op[i]].apply(percent_row, axis=1)],axis=1)\n",
    "    assert( all(train_df.columns == test_df.columns))\n",
    "\n",
    "\n",
    "\n",
    "    # num_op\n",
    "    num_op = [\n",
    "            ['num_op_var39_hace2','num_op_var40_hace2','num_op_var41_hace2'],\n",
    "            ['num_op_var39_hace3','num_op_var40_hace3','num_op_var41_hace3'],\n",
    "            ['num_op_var39_ult1','num_op_var40_ult1','num_op_var41_ult1'],\n",
    "            ['num_op_var39_ult3','num_op_var40_ult1','num_op_var40_hace2','num_op_var40_hace3','num_op_var41_ult1','num_op_var41_hace2','num_op_var41_hace3'],\n",
    "            ]\n",
    "\n",
    "    def percent_row(data):\n",
    "        #print data\n",
    "        lhs = data.index[0]\n",
    "        sum_ = data[1:].sum()\n",
    "        data.index += '_pct_num_op_' + lhs\n",
    "        return pd.Series(data[1:])/(sum_+0.1)\n",
    "\n",
    "    for i in range(len(num_op)):\n",
    "        train_df = pd.concat([train_df, ori_train[num_op[i]].apply(percent_row, axis=1)], axis=1)\n",
    "        test_df = pd.concat([test_df, ori_test[num_op[i]].apply(percent_row, axis=1)], axis=1)\n",
    "    assert( all(train_df.columns == test_df.columns))\n",
    "\n",
    "\n",
    "    #make features of var39, var40, and var41\n",
    "    varl = ['var39', 'var40', 'var41']\n",
    "    for i in varl:\n",
    "        #train\n",
    "        #imp_op_var39_ult1\n",
    "        train_df['imp_op_{}_efect_ult1/imp_op_{}_ult1'.format(i,i)] = ori_train['imp_op_{}_efect_ult1'.format(i)] / (ori_train['imp_op_{}_ult1'.format(i)] + 0.01)\n",
    "        train_df['imp_op_{}_comer_ult1/imp_op_{}_ult1'.format(i,i)] = ori_train['imp_op_{}_comer_ult1'.format(i)] / (ori_train['imp_op_{}_ult1'.format(i)] + 0.01)\n",
    "        train_df['imp_op_{}_efect_ult1/imp_op_{}_comer_ult1'.format(i,i)] = ori_train['imp_op_{}_efect_ult1'.format(i)] / (ori_train['imp_op_{}_comer_ult1'.format(i)] + 0.01)\n",
    "\n",
    "        #train_df['imp_op_{}_efect_ult3/imp_op_{}_ult3'.format(i,i)] = ori_train['imp_op_{}_efect_ult3'.format(i)] / (ori_train['imp_op_{}_ult3'.format(i)] + 0.01)\n",
    "        #train_df['imp_op_{}_comer_ult3/imp_op_{}_ult3'.format(i,i)] = ori_train['imp_op_{}_comer_ult3'.format(i)] / (ori_train['imp_op_{}_ult3'.format(i)] + 0.01)\n",
    "        train_df['imp_op_{}_efect_ult3/imp_op_{}_comer_ult3'.format(i,i)] = ori_train['imp_op_{}_efect_ult3'.format(i)] / (ori_train['imp_op_{}_comer_ult3'.format(i)] + 0.01)\n",
    "\n",
    "        #num_op_var39_ult1\n",
    "        train_df['num_op_{}_efect_ult1/num_op_{}_ult1'.format(i,i)] = ori_train['num_op_{}_efect_ult1'.format(i)] / (ori_train['num_op_{}_ult1'.format(i)] + 0.01)\n",
    "        train_df['num_op_{}_comer_ult1/num_op_{}_ult1'.format(i,i)] = ori_train['num_op_{}_comer_ult1'.format(i)] / (ori_train['num_op_{}_ult1'.format(i)] + 0.01)\n",
    "        train_df['num_op_{}_efect_ult1/num_op_{}_comer_ult1'.format(i,i)] = ori_train['num_op_{}_efect_ult1'.format(i)] / (ori_train['num_op_{}_comer_ult1'.format(i)] + 0.01)\n",
    "\n",
    "        train_df['num_op_{}_efect_ult3/num_op_{}_ult3'.format(i,i)] = ori_train['num_op_{}_efect_ult3'.format(i)] / (ori_train['num_op_{}_ult3'.format(i)] + 0.01)\n",
    "        train_df['num_op_{}_comer_ult3/num_op_{}_ult3'.format(i,i)] = ori_train['num_op_{}_comer_ult3'.format(i)] / (ori_train['num_op_{}_ult3'.format(i)] + 0.01)\n",
    "        train_df['num_op_{}_efect_ult3/num_op_{}_comer_ult3'.format(i,i)] = ori_train['num_op_{}_efect_ult3'.format(i)] / (ori_train['num_op_{}_comer_ult3'.format(i)] + 0.01)\n",
    "\n",
    "        #num/imp_op_var39_ult1\n",
    "        train_df['imp_op_{}_efect_ult1/num_op_{}_ult1'.format(i,i)] = ori_train['imp_op_{}_efect_ult1'.format(i)] / (ori_train['num_op_{}_ult1'.format(i)] + 0.01)\n",
    "        train_df['imp_op_{}_comer_ult1/num_op_{}_ult1'.format(i,i)] = ori_train['imp_op_{}_comer_ult1'.format(i)] / (ori_train['num_op_{}_ult1'.format(i)] + 0.01)\n",
    "        train_df['imp_op_{}_efect_ult1/num_op_{}_comer_ult1'.format(i,i)] = ori_train['imp_op_{}_efect_ult1'.format(i)] / (ori_train['num_op_{}_comer_ult1'.format(i)] + 0.01)\n",
    "\n",
    "        train_df['imp_op_{}_efect_ult3/num_op_{}_ult3'.format(i,i)] = ori_train['imp_op_{}_efect_ult3'.format(i)] / (ori_train['num_op_{}_ult3'.format(i)] + 0.01)\n",
    "        train_df['imp_op_{}_comer_ult3/num_op_{}_ult3'.format(i,i)] = ori_train['imp_op_{}_comer_ult3'.format(i)] / (ori_train['num_op_{}_ult3'.format(i)] + 0.01)\n",
    "        train_df['imp_op_{}_efect_ult3/num_op_{}_comer_ult3'.format(i,i)] = ori_train['imp_op_{}_efect_ult3'.format(i)] / (ori_train['num_op_{}_comer_ult3'.format(i)] + 0.01)\n",
    "\n",
    "        #test\n",
    "        #imp_op_var39_ult1\n",
    "        test_df['imp_op_{}_efect_ult1/imp_op_{}_ult1'.format(i,i)] = ori_test['imp_op_{}_efect_ult1'.format(i)] / (ori_test['imp_op_{}_ult1'.format(i)] + 0.01)\n",
    "        test_df['imp_op_{}_comer_ult1/imp_op_{}_ult1'.format(i,i)] = ori_test['imp_op_{}_comer_ult1'.format(i)] / (ori_test['imp_op_{}_ult1'.format(i)] + 0.01)\n",
    "        test_df['imp_op_{}_efect_ult1/imp_op_{}_comer_ult1'.format(i,i)] = ori_test['imp_op_{}_efect_ult1'.format(i)] / (ori_test['imp_op_{}_comer_ult1'.format(i)] + 0.01)\n",
    "\n",
    "        #test_df['imp_op_{}_efect_ult3/imp_op_{}_ult3'.format(i,i)] = ori_test['imp_op_{}_efect_ult3'.format(i)] / (ori_test['imp_op_{}_ult3'.format(i)] + 0.01)\n",
    "        #test_df['imp_op_{}_comer_ult3/imp_op_{}_ult3'.format(i,i)] = ori_test['imp_op_{}_comer_ult3'.format(i)] / (ori_test['imp_op_{}_ult3'.format(i)] + 0.01)\n",
    "        test_df['imp_op_{}_efect_ult3/imp_op_{}_comer_ult3'.format(i,i)] = ori_test['imp_op_{}_efect_ult3'.format(i)] / (ori_test['imp_op_{}_comer_ult3'.format(i)] + 0.01)\n",
    "\n",
    "        #num_op_var39_ult1\n",
    "        test_df['num_op_{}_efect_ult1/num_op_{}_ult1'.format(i,i)] = ori_test['num_op_{}_efect_ult1'.format(i)] / (ori_test['num_op_{}_ult1'.format(i)] + 0.01)\n",
    "        test_df['num_op_{}_comer_ult1/num_op_{}_ult1'.format(i,i)] = ori_test['num_op_{}_comer_ult1'.format(i)] / (ori_test['num_op_{}_ult1'.format(i)] + 0.01)\n",
    "        test_df['num_op_{}_efect_ult1/num_op_{}_comer_ult1'.format(i,i)] = ori_test['num_op_{}_efect_ult1'.format(i)] / (ori_test['num_op_{}_comer_ult1'.format(i)] + 0.01)\n",
    "\n",
    "        test_df['num_op_{}_efect_ult3/num_op_{}_ult3'.format(i,i)] = ori_test['num_op_{}_efect_ult3'.format(i)] / (ori_test['num_op_{}_ult3'.format(i)] + 0.01)\n",
    "        test_df['num_op_{}_comer_ult3/num_op_{}_ult3'.format(i,i)] = ori_test['num_op_{}_comer_ult3'.format(i)] / (ori_test['num_op_{}_ult3'.format(i)] + 0.01)\n",
    "        test_df['num_op_{}_efect_ult3/num_op_{}_comer_ult3'.format(i,i)] = ori_test['num_op_{}_efect_ult3'.format(i)] / (ori_test['num_op_{}_comer_ult3'.format(i)] + 0.01)\n",
    "\n",
    "        #num/imp_op_var39_ult1\n",
    "        test_df['imp_op_{}_efect_ult1/num_op_{}_ult1'.format(i,i)] = ori_train['imp_op_{}_efect_ult1'.format(i)] / (ori_train['num_op_{}_ult1'.format(i)] + 0.01)\n",
    "        test_df['imp_op_{}_comer_ult1/num_op_{}_ult1'.format(i,i)] = ori_train['imp_op_{}_comer_ult1'.format(i)] / (ori_train['num_op_{}_ult1'.format(i)] + 0.01)\n",
    "        test_df['imp_op_{}_efect_ult1/num_op_{}_comer_ult1'.format(i,i)] = ori_train['imp_op_{}_efect_ult1'.format(i)] / (ori_train['num_op_{}_comer_ult1'.format(i)] + 0.01)\n",
    "\n",
    "        test_df['imp_op_{}_efect_ult3/num_op_{}_ult3'.format(i,i)] = ori_train['imp_op_{}_efect_ult3'.format(i)] / (ori_train['num_op_{}_ult3'.format(i)] + 0.01)\n",
    "        test_df['imp_op_{}_comer_ult3/num_op_{}_ult3'.format(i,i)] = ori_train['imp_op_{}_comer_ult3'.format(i)] / (ori_train['num_op_{}_ult3'.format(i)] + 0.01)\n",
    "        test_df['imp_op_{}_efect_ult3/num_op_{}_comer_ult3'.format(i,i)] = ori_train['imp_op_{}_efect_ult3'.format(i)] / (ori_train['num_op_{}_comer_ult3'.format(i)] + 0.01)\n",
    "\n",
    "\n",
    "    assert( all(train_df.columns == test_df.columns))\n",
    "\n",
    "\n",
    "\n",
    "    # saldo_var13 = 1 * saldo_var13_corto + 1 * saldo_var13_largo + 1 * saldo_var13_medio #\n",
    "    saldo = [\n",
    "            ['saldo_var13','saldo_var13_corto','saldo_var13_medio','saldo_var13_largo'],\n",
    "            ]\n",
    "\n",
    "    def percent_row(data):\n",
    "        #print data\n",
    "        lhs = data.index[0]\n",
    "        sum_ = data.sum()\n",
    "        data.index += '_pct_saldo_' + lhs \n",
    "        return pd.Series(data)/(sum_+0.1)\n",
    "\n",
    "    train_df = pd.concat([train_df, ori_train[saldo[0]].apply(percent_row, axis=1)],axis=1)\n",
    "    test_df = pd.concat([test_df, ori_test[saldo[0]].apply(percent_row, axis=1)],axis=1)\n",
    "    assert( all(train_df.columns == test_df.columns))\n",
    "\n",
    "\n",
    "\n",
    "    #num_var\n",
    "    num_var = [\n",
    "            ['num_var13_0','num_var13_corto_0','ind_var13_medio_0','num_var13_largo_0'],\n",
    "            ['num_var13','num_var13_corto','ind_var13_medio','num_var13_largo'],\n",
    "            ]\n",
    "\n",
    "    def percent_row(data):\n",
    "        #print data\n",
    "        lhs = data.index[0]\n",
    "        sum_ = data.sum()\n",
    "        data.index += '_pct_num_var' + lhs\n",
    "        return pd.Series(data)/(sum_+0.1)\n",
    "\n",
    "    for i in range(len(num_var)):\n",
    "        train_df = pd.concat([train_df, ori_train[num_var[i]].apply(percent_row, axis=1)],axis=1)\n",
    "        test_df = pd.concat([test_df, ori_test[num_var[i]].apply(percent_row, axis=1)],axis=1)\n",
    "\n",
    "    assert( all(train_df.columns == test_df.columns))\n",
    "\n",
    "    ori_train = pd.concat([ori_train, train_df], axis=1)\n",
    "    ori_test = pd.concat([ori_test, test_df], axis=1)\n",
    "    \n",
    "\n",
    "    return ori_train, ori_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 378,
     "status": "ok",
     "timestamp": 1639199699993,
     "user": {
      "displayName": "Huy Lê",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "04255824725604749089"
     },
     "user_tz": -420
    },
    "id": "R_gy34ZB3zK7"
   },
   "outputs": [],
   "source": [
    "def main_feat(train, test, sample_submit=None):\n",
    "    \n",
    "    train_target = train['target']\n",
    "    del train['target']\n",
    "\n",
    "    #delete id\n",
    "    del train['t_id'], test['t_id']\n",
    "\n",
    "    # 0 count per ID\n",
    "    def countZero(data):\n",
    "        return np.sum(data == 0)\n",
    "\n",
    "    train['count0'] = train.apply(countZero, axis=1)\n",
    "    test['count0'] = test.apply(countZero, axis=1)\n",
    "\n",
    "    # add count features of integer columns\n",
    "    int_col = (train.dtypes == int)[(train.dtypes == int).values].index\n",
    "    train_test = pd.concat([train,test])\n",
    "    for i in int_col:\n",
    "        tmp_cnt = train_test[i].value_counts()\n",
    "        tmp_cnt = tmp_cnt.to_frame(name=i+'_cnt')\n",
    "        tmp_cnt[i] = tmp_cnt.index\n",
    "        tmp_cnt.reset_index(drop=True, inplace=True)\n",
    "        train = train.reset_index().merge(tmp_cnt, how='left', on=i).sort('index').drop('index', axis=1)\n",
    "        test = test.reset_index().merge(tmp_cnt, how='left', on=i).sort('index').drop('index', axis=1)\n",
    "        train.reset_index(drop=True, inplace=True)\n",
    "        test.reset_index(drop=True, inplace=True)\n",
    "    del train_test\n",
    "\n",
    "    # make dummy variables of var3 in the threshold(>=5)\n",
    "    var3_cnt = train.var3.value_counts()\n",
    "    index_var3_th = var3_cnt[(var3_cnt>=5).values].index\n",
    "    train['var3_tmp'] = train.var3.apply(lambda x: x if x in index_var3_th else np.nan)\n",
    "    test['var3_tmp'] = test.var3.apply(lambda x: x if x in index_var3_th else np.nan)\n",
    "    \n",
    "    train_test = pd.concat([train,test])\n",
    "    #train_test.reset_index(drop=True, inplace=True)\n",
    "    tmp = pd.get_dummies(train_test['var3_tmp'], prefix='ohe_var3', prefix_sep='_')\n",
    "\n",
    "    train = pd.concat([train, tmp.iloc[:len(train),:]], axis=1)\n",
    "    test = pd.concat([test, tmp.iloc[len(train):,:]], axis=1)\n",
    "    del train['var3_tmp'], test['var3_tmp']\n",
    "\n",
    "    # add feature of var38\n",
    "    train['var38mc'] = np.isclose(train.var38, 117310.979016)\n",
    "    train['logvar38'] = train.loc[~train['var38mc'], 'var38'].map(np.log)\n",
    "    train.loc[train['var38mc'], 'logvar38'] = 0\n",
    "\n",
    "    test['var38mc'] = np.isclose(test.var38, 117310.979016)\n",
    "    test['logvar38'] = test.loc[~test['var38mc'], 'var38'].map(np.log)\n",
    "    test.loc[test['var38mc'], 'logvar38'] = 0\n",
    "\n",
    "    train['var38mc'] = train['var38mc'].astype(int)\n",
    "\n",
    "    test['var38mc'] = test['var38mc'].astype(int)\n",
    "\n",
    "    #delete constant features\n",
    "    for i in train.columns:\n",
    "        if len(set(train[i].values)) == 1:\n",
    "            del train[i], test[i]\n",
    "    assert( all(train.columns == test.columns))\n",
    "\n",
    "    #delete identical columns\n",
    "    unique_col = train.T.drop_duplicates().T.columns\n",
    "    train = train[unique_col]\n",
    "    test = test[unique_col]\n",
    "    assert( all(train.columns == test.columns))\n",
    "\n",
    "\n",
    "    train['target'] = train_target\n",
    "\n",
    "    train.to_csv('ikki_features_train_ver1.csv',index=None)\n",
    "    test.to_csv('ikki_features_test_ver1.csv',index=None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 339,
     "status": "ok",
     "timestamp": 1639199248141,
     "user": {
      "displayName": "Huy Lê",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "04255824725604749089"
     },
     "user_tz": -420
    },
    "id": "SDGnpY8_32RW"
   },
   "outputs": [],
   "source": [
    "def one_hot_encoder(train, test):\n",
    "\n",
    "    ohe_col = ['num_var13_corto','num_var13_corto_0','num_meses_var12_ult3','num_meses_var13_corto_ult3','num_meses_var39_vig_ult3','num_meses_var5_ult3','num_var24_0','num_var12','var36','num_var5','num_var5_0','num_var12_0','num_var13','num_var13_0','num_var42','num_var4','num_var42_0','num_var30','num_var39_0','num_var41_0']\n",
    "    \n",
    "    train_test = pd.concat([train,test])\n",
    "    train_test.reset_index(drop=True, inplace=True)\n",
    "    ohe_data = pd.DataFrame()\n",
    "    for i in train_test.columns:\n",
    "        if i in ohe_col:\n",
    "            tmp = pd.get_dummies(train_test[i], prefix='ohe_'+i, prefix_sep='_')\n",
    "            ohe_data = pd.concat([ohe_data, tmp], axis=1)\n",
    "    \n",
    "    train = ohe_data.iloc[:len(train),:]\n",
    "    test = ohe_data.iloc[len(train):,:]\n",
    "\n",
    "    train.to_csv('ikki_one_hot_encoder_train_ver1.csv',index=None)\n",
    "    test.to_csv('ikki_one_hot_encoder_test_ver1.csv',index=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 340
    },
    "executionInfo": {
     "elapsed": 316722,
     "status": "error",
     "timestamp": 1639200020856,
     "user": {
      "displayName": "Huy Lê",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "04255824725604749089"
     },
     "user_tz": -420
    },
    "id": "0d4g-DGR35Js",
    "outputId": "4db543ca-d107-44bb-ec66-829233a34aa2"
   },
   "outputs": [],
   "source": [
    "main_feat(train=ori_train.copy(), test=ori_test.copy())\n",
    "one_hot_encoder(train=ori_train.copy(), test=ori_test.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMbvHu5bbbbZhzjqIOlLz3L",
   "collapsed_sections": [],
   "name": "Untitled0.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
