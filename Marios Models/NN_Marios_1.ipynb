{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NN_Marios_1.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOVvVVkyve9VXSPeCfwbnib",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/linh0303052/AplliedDSGroup11/blob/main/Marios%20Models/NN_Marios_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TvLxe5iOzmf5"
      },
      "source": [
        "import numpy as np\n",
        "import joblib\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction import DictVectorizer\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from collections import defaultdict\n",
        "from keras.layers.advanced_activations import PReLU\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from keras.regularizers import *\n",
        "from sklearn.utils import shuffle\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Dense, Dropout, Activation\n",
        "from keras.utils import np_utils\n",
        "from keras import optimizers\n",
        "import keras"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D8EJld1P3oj8"
      },
      "source": [
        "epc=100\n",
        "batch=512"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YZVF00Uv3pA0"
      },
      "source": [
        "def build_model(input_dim, output_dim):\n",
        "    models = Sequential()\n",
        "\n",
        "    models.add(Dense(120, input_dim=input_dim, init='uniform', W_regularizer=l2(0.00001)))\n",
        "    models.add(PReLU())\n",
        "    models.add(BatchNormalization())\n",
        "    models.add(Dropout(0.6))\n",
        "    models.add(Dense(output_dim, init='uniform'))\n",
        "    models.add(Activation('softmax'))\n",
        "\n",
        "    opt = optimizers.Adagrad(lr=0.0125)\n",
        "    models.compile(loss='binary_crossentropy', optimizer=opt)\n",
        "    return models\n",
        "\n",
        "def loadcolumn(filename,col=4, skip=1, floats=True):\n",
        "    pred=[]\n",
        "    op=open(filename,'r')\n",
        "    if skip==1:\n",
        "        op.readline() #header\n",
        "    for line in op:\n",
        "        line=line.replace('\\n','')\n",
        "        sps=line.split(',')\n",
        "        #load always the last columns\n",
        "        if floats:\n",
        "            pred.append(float(sps[col]))\n",
        "        else :\n",
        "            pred.append(str(sps[col]))\n",
        "    op.close()\n",
        "    return np.array(pred)    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xrkpAiNC3vTz"
      },
      "source": [
        "def load_datas(filename):\n",
        "\n",
        "    return joblib.load(filename)\n",
        "\n",
        "def printfile(X, filename):\n",
        "\n",
        "    joblib.dump((X), filename)\n",
        "    \n",
        "def printfilcsve(X, filename, headers):\n",
        "\n",
        "    np.savetxt(filename,X, header=headers) \n",
        "\n",
        "    \n",
        "    \n",
        "def load_ids(id_file, cols=20):\n",
        "    verybiglist=[]\n",
        "    for s in range(0,cols):\n",
        "        idss=loadcolumn(id_file,col=s, skip=1, floats=True)\n",
        "        id_list=[ [] ,[] , [], [] , []]\n",
        "        id_dict=[ defaultdict(int) ,defaultdict(int) , defaultdict(int), defaultdict(int) , defaultdict(int)]\n",
        "        for g in range(0,len(idss)):\n",
        "            id_list[int(idss[g])].append(g)\n",
        "            id_dict[int(idss[g])][g]=1\n",
        "        biglist=[]\n",
        "        for k in range(5):\n",
        "            training_ids=[s for s in range(0,len(idss)) if s not in id_dict[k] ]\n",
        "            biglist.append([training_ids,id_list[k] ])\n",
        "            print(len(biglist), len(biglist[0]))\n",
        "        verybiglist.append(biglist)\n",
        "            \n",
        "    return verybiglist"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-eXOFJdY3zS7"
      },
      "source": [
        "def all_load_vecorizerr(tr,te,drop=[\"ind_var2_0\",\"ind_var2\",\"ind_var27_0\",\"ind_var28_0\",\"ind_var28\",\"ind_var27\",\n",
        "\"ind_var41\",\"ind_var46_0\",\"ind_var46\",\"num_var27_0\",\"num_var28_0\",\"num_var28\",\"num_var27\",\"num_var41\",\"num_var46_0\",\n",
        "\"num_var46\",\"saldo_var28\",\"saldo_var27\",\"saldo_var41\",\"saldo_var46\",\"imp_amort_var18_hace3\",\"imp_amort_var34_hace3\",\n",
        "\"imp_reemb_var13_hace3\",\"imp_reemb_var33_hace3\",\"imp_trasp_var17_out_hace3\",\"imp_trasp_var33_out_hace3\",\n",
        "\"num_var2_0_ult1\",\"num_var2_ult1\",\"num_reemb_var13_hace3\",\"num_reemb_var33_hace3\",\"num_trasp_var17_out_hace3\",\n",
        "\"num_trasp_var33_out_hace3\",\"saldo_var2_ult1\",\"saldo_medio_var13_medio_hace3\",\"ind_var6_0\",\"ind_var6\",\n",
        "\"ind_var13_medio_0\",\"ind_var18_0\",\"ind_var26_0\",\"ind_var25_0\",\"ind_var32_0\",\"ind_var34_0\",\"ind_var37_0\",\n",
        "\"ind_var40\",\"num_var6_0\",\"num_var6\",\"num_var13_medio_0\",\"num_var18_0\",\"num_var26_0\",\"num_var25_0\",\"num_var32_0\",\n",
        "\"num_var34_0\",\"num_var37_0\",\"num_var40\",\"saldo_var6\",\"saldo_var13_medio\",\"delta_imp_reemb_var13_1y3\",\n",
        "\"delta_imp_reemb_var17_1y3\",\"delta_imp_reemb_var33_1y3\",\"delta_imp_trasp_var17_in_1y3\",\"delta_imp_trasp_var17_out_1y3\",\n",
        "\"delta_imp_trasp_var33_in_1y3\",\"delta_imp_trasp_var33_out_1y3\"]):\n",
        "\n",
        "    train  = pd.read_csv(tr, sep=',',quotechar='\"')\n",
        "    test  = pd.read_csv(te, sep=',',quotechar='\"')\n",
        "    train.drop('ID', axis=1, inplace=True)\n",
        "    train.drop('TARGET', axis=1, inplace=True)    \n",
        "    test.drop('ID', axis=1, inplace=True)\n",
        "    for name in drop:\n",
        "        train.drop(name, axis=1, inplace=True)    \n",
        "        test.drop(name, axis=1, inplace=True)        \n",
        "\n",
        "    train['zerocount'] = train.apply(lambda x: np.sum(x == 0), axis=1)\n",
        "    test['zerocount'] = test.apply(lambda x: np.sum(x == 0), axis=1)\n",
        "\n",
        "    train['var38'].replace(117310.979016494, -1.0, inplace=True)\n",
        "    test ['var38'].replace(117310.979016494, -1.0, inplace=True)\n",
        "    \n",
        "    train_s = train\n",
        "    test_s = test\n",
        "    result = pd.concat([test_s,train_s])\n",
        "    \n",
        "    #test_s.drop('id', axis=1, inplace=True)\n",
        "    result=result.T.to_dict().values()\n",
        "    train = train_s.T.to_dict().values()\n",
        "    test = test_s.T.to_dict().values()\n",
        "    \n",
        "    vec = DictVectorizer()\n",
        "    vec.fit(result)\n",
        "    train = vec.transform(train)\n",
        "    test = vec.transform(test)\n",
        "    \n",
        "    print train.shape\n",
        "    print test.shape    \n",
        "    \n",
        "    \n",
        "    return train,test\n",
        "\n",
        "def bagged_set(X,y, seed, estimators, xt, nval=0.0, verbos=0): \n",
        "  \n",
        "   baggedpred=[ 0.0 for d in range(0, xt.shape[0])] \n",
        "           \n",
        "   for i in range (0, estimators): \n",
        "        \n",
        "        X_t,y_c=shuffle(X,y, random_state=seed+i) \n",
        "        np.random.seed(seed+i)\n",
        "        model = build_model(xt.shape[1], 2)\n",
        "        if nval>0.0:\n",
        "            x_train_oof, x_valid_oof, y_train_oof_nn, y_valid_oof_nn = cross_validation.train_test_split(\n",
        "            X_t, y_c, test_size=nval, random_state=i*seed)\n",
        "            model.fit(x_train_oof,np_utils.to_categorical( y_train_oof_nn),\n",
        "                      nb_epoch=epc, batch_size=batch,\n",
        "                      validation_data=(x_valid_oof,np_utils.to_categorical(y_valid_oof_nn)), \n",
        "                    verbose=verbos, callbacks=[MonitorAUC(x_valid_oof,y_valid_oof_nn)])\n",
        "        else :\n",
        "            y_train_oof_nn = np_utils.to_categorical(y_c)\n",
        "            model.fit(X_t, y_train_oof_nn, nb_epoch=epc, batch_size=batch, verbose=verbos)\n",
        "\n",
        "         \n",
        "        preds =model.predict_proba(xt) [:,1]\n",
        " \n",
        "        for j in range (0, xt.shape[0]):            \n",
        "                 baggedpred[j]+=preds[j] \n",
        "        #print(\"finshed bag %d\" % (i+1)) \n",
        "                \n",
        "   for j in range (0, len(baggedpred)):         \n",
        "                baggedpred[j]/=float(estimators) \n",
        "                \n",
        "   return np.array(baggedpred) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XIuS7NFE32yD"
      },
      "source": [
        "load_data=True      \n",
        "metafolder_train=\"../data/output/train/\" \n",
        "metafolder_test=\"../data/output/test/\"        \n",
        "input_folder=\"../data/input/\"\n",
        "feature_folder=\"../data/output/features/\"\n",
        "SEED=15\n",
        "outset=\"nn_marios_1\" # predic of all files\n",
        "number_of_folds=5 # repeat the CV procedure 10 times to get more precise results       \n",
        "\n",
        "######### Load files ############\n",
        "\n",
        "y=loadcolumn(input_folder+ \"train.csv\",col=370, skip=1, floats=True)\n",
        "ids=loadcolumn(input_folder+ \"test.csv\",col=0, skip=1, floats=True)\n",
        "idstrain=loadcolumn(input_folder+ \"train.csv\",col=0, skip=1, floats=True)\n",
        "keepfold=[0 for k in range(len(y))]\n",
        "\n",
        "if load_data:\n",
        "    X,X_test=all_load_vecorizerr(input_folder+'train.csv',input_folder+'test.csv') \n",
        "    printfile(X,\"Xvector.pkl\")  \n",
        "    printfile(X_test,\"Xtestvector.pkl\")                               \n",
        "    X=load_datas(\"Xvector.pkl\").toarray()\n",
        "    X_test=load_datas(\"Xtestvector.pkl\").toarray()\n",
        "\n",
        "\n",
        "tsn_features=(np.loadtxt(feature_folder+ \"tsne_feats.csv\", delimiter=\",\", skiprows=1, usecols=[1,2]))\n",
        "\n",
        "tsn_features_train=tsn_features[:X.shape[0]]\n",
        "tsn_features_test=tsn_features[X.shape[0]:tsn_features.shape[0]]     \n",
        "\n",
        "print(tsn_features_train.shape)\n",
        "print(tsn_features_test.shape)\n",
        "\n",
        "X=np.column_stack((X,tsn_features_train))     \n",
        "X_test=np.column_stack((X_test,tsn_features_test)) \n",
        "\n",
        "print(X.shape)\n",
        "print(X_test.shape)\n",
        "\n",
        "pca_features=(np.loadtxt(feature_folder+\"dmitry_pca_feats.csv\", delimiter=\",\", skiprows=1, usecols=[1,2]))\n",
        "\n",
        "dmitry_pca_feats_train=pca_features[:X.shape[0]]\n",
        "dmitry_pca_feats_test=pca_features[X.shape[0]:pca_features.shape[0]]     \n",
        "\n",
        "print(dmitry_pca_feats_train.shape)\n",
        "print(dmitry_pca_feats_test.shape)\n",
        "\n",
        "X=np.column_stack((X,dmitry_pca_feats_train))     \n",
        "X_test=np.column_stack((X_test,dmitry_pca_feats_test)) \n",
        "\n",
        "print(X.shape)     \n",
        "print(X_test.shape)\n",
        "\n",
        "\n",
        "#Create Arrays for meta\n",
        "train_stacker=[ 0.0  for k in range (0,(X.shape[0])) ]\n",
        "test_stacker=[0.0  for k in range (0,(X_test.shape[0]))]\n",
        "\n",
        "# CHECK EVerything in five..it could be more efficient     \n",
        "\n",
        "#create target variable        \n",
        "\n",
        "#kfolder=StratifiedKFold(y, n_folds=number_of_folds,shuffle=True, random_state=SEED)\n",
        "print(\"kfolder\")\n",
        "\n",
        "#load the 20-fold ids.        \n",
        "kfolders=load_ids(input_folder+\"5fold_20times.csv\")  \n",
        "\n",
        "printfile(kfolders,\"kfolder.pkl\")                   \n",
        "kfolders=load_datas(\"kfolder.pkl\")\n",
        "\n",
        "fcount=0\n",
        "#number_of_folds=0\n",
        "#X,y=shuffle(X,y, random_state=SEED) # Shuffle since the data is ordered by time\n",
        "for kfolder in kfolders:\n",
        "    mean_kapa = 0.0\n",
        "    i=0 # iterator counter\n",
        "    print (\"starting cross validation with %d kfolds \" % (number_of_folds))\n",
        "    if number_of_folds>0:\n",
        "        for train_index, test_index in kfolder:\n",
        "            # creaning and validation sets\n",
        "            X_train, X_cv = X[train_index], X[test_index]\n",
        "            y_train, y_cv = np.array(y)[train_index], np.array(y)[test_index]\n",
        "\n",
        "            stda=StandardScaler()            \n",
        "            X_train=stda.fit_transform(X_train)\n",
        "            X_cv=stda.transform(X_cv)                    \n",
        "\n",
        "            print(\"folder %d  train size: %d. test size: %d, cols: %d \" % (fcount, (X_train.shape[0]) ,(X_cv.shape[0]) ,(X_train.shape[1]) ))\n",
        "\n",
        "            \n",
        "            preds=bagged_set(X_train,y_train, SEED, 5, X_cv)\n",
        "            \n",
        "            # compute Loglikelihood metric for this CV fold     \n",
        "            kapa = roc_auc_score(y_cv,preds)\n",
        "            print(\"folder %d size train: %d size cv: %d AUC (fold %d/%d): %f\" % (fcount,(X_train.shape[0]), (X_cv.shape[0]), i + 1, number_of_folds, kapa))\n",
        "            mean_kapa += kapa\n",
        "            #save the results\n",
        "            no=0\n",
        "            for real_index in test_index:\n",
        "                      train_stacker[real_index]+=(preds[no])\n",
        "                      keepfold[real_index]=i\n",
        "                      no+=1\n",
        "            i+=1\n",
        "    fcount+=1\n",
        "    print(\"==============================================================================================\")\n",
        "for u in range(0,len(train_stacker)):\n",
        "    train_stacker[u]/=float(len(kfolders))\n",
        "grand_auc=roc_auc_score(y, train_stacker)\n",
        "print (\" Grand AUC: %f\" % (grand_auc) )\n",
        "if (number_of_folds)>0:\n",
        "    mean_kapa/=number_of_folds\n",
        "    print (\" printing train datasets \")\n",
        "    printfilcsve(np.column_stack((np.array(idstrain),np.array(train_stacker))), metafolder_train+ outset  + \".train.csv\",\"ID,TARGET\")  \n",
        "    #printfilcsve(np.column_stack((np.array(idstrain),np.array(keepfold))),   \"id_fold.csv\",\"ID,FOLD\")\n",
        "\n",
        "#woe_train, woe_cv= convert_to_woe(np.round(X,2),y, np.round(X_test,2), seed=1, cvals=5, roundings=2, columns=None)\n",
        "\n",
        "stda=StandardScaler()            \n",
        "X=stda.fit_transform(X)\n",
        "X_test=stda.transform(X_test)           \n",
        "\n",
        "print (\" making test predictions \")        \n",
        "preds=bagged_set(X, y, SEED, 50, X_test) \n",
        "\n",
        "for pr in range (0,len(preds)):            \n",
        "            test_stacker[pr]=(preds[pr]) \n",
        "\n",
        "preds=np.array(preds)\n",
        "printfilcsve(np.column_stack((np.array(ids),np.array(test_stacker))),  metafolder_test+ outset  + \".test.csv\",\"ID,TARGET\")                "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}