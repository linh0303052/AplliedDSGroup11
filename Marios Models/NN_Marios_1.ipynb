{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NN_Marios_1.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOdcPEumshK5wxR7qZ152ng",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/linh0303052/AplliedDSGroup11/blob/main/Marios%20Models/NN_Marios_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TvLxe5iOzmf5"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.externals import joblib\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction import DictVectorizer\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from collections import defaultdict\n",
        "from keras.layers.advanced_activations import PReLU\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.regularizers import *\n",
        "from sklearn.utils import shuffle\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Dense, Dropout, Activation\n",
        "from keras.utils import np_utils\n",
        "from keras import optimizers\n",
        "import keras"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D8EJld1P3oj8"
      },
      "source": [
        "epc=100\n",
        "batch=512"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YZVF00Uv3pA0"
      },
      "source": [
        "def build_model(input_dim, output_dim):\n",
        "    models = Sequential()\n",
        "\n",
        "    models.add(Dense(120, input_dim=input_dim, init='uniform', W_regularizer=l2(0.00001)))\n",
        "    models.add(PReLU())\n",
        "    models.add(BatchNormalization())\n",
        "    models.add(Dropout(0.6))\n",
        "    models.add(Dense(output_dim, init='uniform'))\n",
        "    models.add(Activation('softmax'))\n",
        "\n",
        "    opt = optimizers.Adagrad(lr=0.0125)\n",
        "    models.compile(loss='binary_crossentropy', optimizer=opt)\n",
        "    return models\n",
        "\n",
        "def loadcolumn(filename,col=4, skip=1, floats=True):\n",
        "    pred=[]\n",
        "    op=open(filename,'r')\n",
        "    if skip==1:\n",
        "        op.readline() #header\n",
        "    for line in op:\n",
        "        line=line.replace('\\n','')\n",
        "        sps=line.split(',')\n",
        "        #load always the last columns\n",
        "        if floats:\n",
        "            pred.append(float(sps[col]))\n",
        "        else :\n",
        "            pred.append(str(sps[col]))\n",
        "    op.close()\n",
        "    return np.array(pred)    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xrkpAiNC3vTz"
      },
      "source": [
        "def load_datas(filename):\n",
        "\n",
        "    return joblib.load(filename)\n",
        "\n",
        "def printfile(X, filename):\n",
        "\n",
        "    joblib.dump((X), filename)\n",
        "    \n",
        "def printfilcsve(X, filename, headers):\n",
        "\n",
        "    np.savetxt(filename,X, header=headers) \n",
        "\n",
        "    \n",
        "    \n",
        "def load_ids(id_file, cols=20):\n",
        "    verybiglist=[]\n",
        "    for s in range(0,cols):\n",
        "        idss=loadcolumn(id_file,col=s, skip=1, floats=True)\n",
        "        id_list=[ [] ,[] , [], [] , []]\n",
        "        id_dict=[ defaultdict(int) ,defaultdict(int) , defaultdict(int), defaultdict(int) , defaultdict(int)]\n",
        "        for g in range(0,len(idss)):\n",
        "            id_list[int(idss[g])].append(g)\n",
        "            id_dict[int(idss[g])][g]=1\n",
        "        biglist=[]\n",
        "        for k in range(5):\n",
        "            training_ids=[s for s in range(0,len(idss)) if s not in id_dict[k] ]\n",
        "            biglist.append([training_ids,id_list[k] ])\n",
        "            print(len(biglist), len(biglist[0]))\n",
        "        verybiglist.append(biglist)\n",
        "            \n",
        "    return verybiglist"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-eXOFJdY3zS7"
      },
      "source": [
        "def all_load_vecorizerr(tr,te,drop=[\"ind_var2_0\",\"ind_var2\",\"ind_var27_0\",\"ind_var28_0\",\"ind_var28\",\"ind_var27\",\n",
        "\"ind_var41\",\"ind_var46_0\",\"ind_var46\",\"num_var27_0\",\"num_var28_0\",\"num_var28\",\"num_var27\",\"num_var41\",\"num_var46_0\",\n",
        "\"num_var46\",\"saldo_var28\",\"saldo_var27\",\"saldo_var41\",\"saldo_var46\",\"imp_amort_var18_hace3\",\"imp_amort_var34_hace3\",\n",
        "\"imp_reemb_var13_hace3\",\"imp_reemb_var33_hace3\",\"imp_trasp_var17_out_hace3\",\"imp_trasp_var33_out_hace3\",\n",
        "\"num_var2_0_ult1\",\"num_var2_ult1\",\"num_reemb_var13_hace3\",\"num_reemb_var33_hace3\",\"num_trasp_var17_out_hace3\",\n",
        "\"num_trasp_var33_out_hace3\",\"saldo_var2_ult1\",\"saldo_medio_var13_medio_hace3\",\"ind_var6_0\",\"ind_var6\",\n",
        "\"ind_var13_medio_0\",\"ind_var18_0\",\"ind_var26_0\",\"ind_var25_0\",\"ind_var32_0\",\"ind_var34_0\",\"ind_var37_0\",\n",
        "\"ind_var40\",\"num_var6_0\",\"num_var6\",\"num_var13_medio_0\",\"num_var18_0\",\"num_var26_0\",\"num_var25_0\",\"num_var32_0\",\n",
        "\"num_var34_0\",\"num_var37_0\",\"num_var40\",\"saldo_var6\",\"saldo_var13_medio\",\"delta_imp_reemb_var13_1y3\",\n",
        "\"delta_imp_reemb_var17_1y3\",\"delta_imp_reemb_var33_1y3\",\"delta_imp_trasp_var17_in_1y3\",\"delta_imp_trasp_var17_out_1y3\",\n",
        "\"delta_imp_trasp_var33_in_1y3\",\"delta_imp_trasp_var33_out_1y3\"]):\n",
        "\n",
        "    train  = pd.read_csv(tr, sep=',',quotechar='\"')\n",
        "    test  = pd.read_csv(te, sep=',',quotechar='\"')\n",
        "    train.drop('ID', axis=1, inplace=True)\n",
        "    train.drop('TARGET', axis=1, inplace=True)    \n",
        "    test.drop('ID', axis=1, inplace=True)\n",
        "    for name in drop:\n",
        "        train.drop(name, axis=1, inplace=True)    \n",
        "        test.drop(name, axis=1, inplace=True)        \n",
        "\n",
        "    train['zerocount'] = train.apply(lambda x: np.sum(x == 0), axis=1)\n",
        "    test['zerocount'] = test.apply(lambda x: np.sum(x == 0), axis=1)\n",
        "\n",
        "    train['var38'].replace(117310.979016494, -1.0, inplace=True)\n",
        "    test ['var38'].replace(117310.979016494, -1.0, inplace=True)\n",
        "    \n",
        "    train_s = train\n",
        "    test_s = test\n",
        "    result = pd.concat([test_s,train_s])\n",
        "    \n",
        "    #test_s.drop('id', axis=1, inplace=True)\n",
        "    result=result.T.to_dict().values()\n",
        "    train = train_s.T.to_dict().values()\n",
        "    test = test_s.T.to_dict().values()\n",
        "    \n",
        "    vec = DictVectorizer()\n",
        "    vec.fit(result)\n",
        "    train = vec.transform(train)\n",
        "    test = vec.transform(test)\n",
        "    \n",
        "    print train.shape\n",
        "    print test.shape    \n",
        "    \n",
        "    \n",
        "    return train,test\n",
        "\n",
        "def bagged_set(X,y, seed, estimators, xt, nval=0.0, verbos=0): \n",
        "  \n",
        "   baggedpred=[ 0.0 for d in range(0, xt.shape[0])] \n",
        "           \n",
        "   for i in range (0, estimators): \n",
        "        \n",
        "        X_t,y_c=shuffle(X,y, random_state=seed+i) \n",
        "        np.random.seed(seed+i)\n",
        "        model = build_model(xt.shape[1], 2)\n",
        "        if nval>0.0:\n",
        "            x_train_oof, x_valid_oof, y_train_oof_nn, y_valid_oof_nn = cross_validation.train_test_split(\n",
        "            X_t, y_c, test_size=nval, random_state=i*seed)\n",
        "            model.fit(x_train_oof,np_utils.to_categorical( y_train_oof_nn),\n",
        "                      nb_epoch=epc, batch_size=batch,\n",
        "                      validation_data=(x_valid_oof,np_utils.to_categorical(y_valid_oof_nn)), \n",
        "                    verbose=verbos, callbacks=[MonitorAUC(x_valid_oof,y_valid_oof_nn)])\n",
        "        else :\n",
        "            y_train_oof_nn = np_utils.to_categorical(y_c)\n",
        "            model.fit(X_t, y_train_oof_nn, nb_epoch=epc, batch_size=batch, verbose=verbos)\n",
        "\n",
        "         \n",
        "        preds =model.predict_proba(xt) [:,1]\n",
        " \n",
        "        for j in range (0, xt.shape[0]):            \n",
        "                 baggedpred[j]+=preds[j] \n",
        "        #print(\"finshed bag %d\" % (i+1)) \n",
        "                \n",
        "   for j in range (0, len(baggedpred)):         \n",
        "                baggedpred[j]/=float(estimators) \n",
        "                \n",
        "   return np.array(baggedpred) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XIuS7NFE32yD"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}