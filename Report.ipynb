{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b2dcf7c",
   "metadata": {},
   "source": [
    "# Báo cáo đồ án môn \"Khoa học dữ liệu ứng dụng\"\n",
    "\n",
    "Nhóm 11:\n",
    "1. 1612355 - Lê Kinh Luân - https://github.com/kluan98\n",
    "2. 1712284 - Hoàng Gia Bảo - https://github.com/hgbao8799\n",
    "3. 18120052 - Lê Hạnh Linh - https://github.com/linh0303052\n",
    "4. 18120182 - Lê Hồng Huy - https://github.com/lehonghuy2000\n",
    "5. 1712371 - Nguyễn Văn Dưng - https://github.com/dungdev1\n",
    "\n",
    "Link thùng chứa Github của nhóm: https://github.com/linh0303052/AplliedDSGroup11"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f93d974",
   "metadata": {},
   "source": [
    "## Mô tả bài toán"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0bf646",
   "metadata": {},
   "source": [
    "Bài toán **Santander Customer Satisfaction** được tổ chức bởi Ngân hàng Santander có thị trường là Đông Bắc Hoa Kỳ và là công ty con của tập đoàn Santander ở Tây Ban Nha.\n",
    "\n",
    "**Link:** https://www.kaggle.com/c/santander-customer-satisfaction/overview\n",
    "\n",
    "**Vấn đề:** Phân tích sự hài lòng của khách hàng là phép đo chìa khóa của thành công bởi vì các khách hàng không hài lòng sẽ có ít cơ hội mua lại lần tiếp theo và quan trọng hơn là họ sẽ nói lên sự không hài lòng của họ trên các nền tảng xã hội trước khi rời đi. Ngân hàng Santander mong muốn phát hiện sớm các khách hàng không hài lòng và có các giải pháp chủ động để cải thiện trải nghiệm, thái độ của khách hàng trước khi quá muộn.\n",
    "\n",
    "**Input:** Dữ liệu gồm 370 thuộc tính được anonymized\n",
    "\n",
    "**Output:** Dự đoán đây có phải là khách hàng không hài lòng với kết quả là 1 nếu *không hài lòng* hoặc 0 nếu *hài lòng*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d48122eb",
   "metadata": {},
   "source": [
    "## Giải quyết bài toán"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f8cfd2",
   "metadata": {},
   "source": [
    "Trong đồ án này, nhóm tìm hiểu và cài đặt lại phương pháp của nhóm **#1 Leustagos** đặt hạng 3 trên Private Leaderboard\n",
    "\n",
    "**Github:** https://github.com/diefimov/santander_2016\n",
    "\n",
    "**Mô tả:** Giải pháp của tác giả chủ yếu dựa trên kết hợp (ensemble) của các mô hình khác nhau vì dataset gồm rất nhiều thuộc tính nên việc dùng ensemble để tránh tình trạng overfit. Các tác giả sử dụng khá đa dạng các mô hình có tính chất làm việc tốt trên dataset bị ẩn danh hóa và dạng bảng như Random Forest, Adaboost Classifier, XGBOOST, Neural Network. Mỗi nhóm mô hình có cách tiền xử lý dữ liệu khác nhau dành cho mỗi mô hình thay vì chỉ tiền xử lý một lần đầu và dùng cho tất cả mô hình. Đây là lí do nhóm chọn giải pháp của nhóm tác giả này trong số các giải pháp top đầu.\n",
    "\n",
    "Giải pháp này là một ensemble của 5 nhóm mô hình lớn, được tiền xử lý và chạy độc lập với nhau. Đặc điểm này cũng quy định cấu trúc của phần tiếp theo: mỗi nhóm mô hình sẽ được chia thành 2 phần lớn là phần *tiền xử lý dữ liệu* và phần *mô hình*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a92589",
   "metadata": {},
   "source": [
    "### Import các thư viện cần thiết"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6b0d16d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from scipy.stats.stats import pearsonr\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5700124",
   "metadata": {},
   "source": [
    "### Tải dữ liệu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bd377723",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(76020, 371)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('./data/input/train.csv')\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e675b798",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(75818, 370)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.read_csv('./data/input/test.csv')\n",
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9b0e305d",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_PATH = './data/input/'\n",
    "OUTPUT_PATH = './data/output/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b201ca6f",
   "metadata": {},
   "source": [
    "### Nhóm mô hình Dmitry Efimov"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b600fa81",
   "metadata": {},
   "source": [
    "#### Tiền xử lý dữ liệu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6a3f4482",
   "metadata": {},
   "outputs": [],
   "source": [
    "#xử lý các giá trị đặc biệt, thay thế nó bằng giá trị NA (-999.0)\n",
    "train.loc[(train['var38']>117310.979) & (train['var38']<117310.98), 'var38'] = -999.0\n",
    "test.loc[(test['var38']>117310.979) & (test['var38']<117310.98), 'var38'] = -999.0\n",
    "\n",
    "train.loc[train['var3']==-999999, 'var3'] = -999.0\n",
    "test.loc[test['var3']==-999999, 'var3'] = -999.0\n",
    "\n",
    "for f in ['imp_op_var40_comer_ult1', 'imp_op_var40_efect_ult3', 'imp_op_var41_comer_ult3', 'imp_sal_var16_ult1']:\n",
    "    train.loc[train[f]==0.0, f] = -999.0\n",
    "    test.loc[test[f]==0.0, f] = -999.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f59a966c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loại bỏ các thuộc tính chỉ có 1 giá trị\n",
    "flist = [x for x in train.columns if not x in ['ID','TARGET']]\n",
    "for f in flist:\n",
    "    if len(np.unique(train[f]))<2:\n",
    "        train.drop(f, axis=1, inplace=True)\n",
    "        test.drop(f, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9a3788d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loại bỏ var6 vì nó trùng với var29\n",
    "flist = [x for x in train.columns if not x in ['ID','TARGET']]            \n",
    "train.drop([x for x in flist if 'var6' in x], axis=1, inplace=True)\n",
    "test.drop([x for x in flist if 'var6' in x], axis=1, inplace=True)\n",
    "\n",
    "#Loại bỏ các thuộc tính có chứa _0 vì nó bị trùng với cột có chứa _1 theo ngay sau\n",
    "flist = [x for x in train.columns if not x in ['ID','TARGET']]        \n",
    "flist_remove = []\n",
    "for i in range(len(flist)-1):\n",
    "    v = train[flist[i]].values\n",
    "    for j in range(i+1, len(flist)):\n",
    "        if np.array_equal(v, train[flist[j]].values):\n",
    "            if '_0' in flist[j]:\n",
    "                flist_remove.append(flist[j])\n",
    "            elif  '_0' in flist[i]:\n",
    "                flist_remove.append(flist[i])\n",
    "train.drop(flist_remove, axis=1, inplace=True)\n",
    "test.drop(flist_remove, axis=1, inplace=True)\n",
    "\n",
    "#Loại bỏ các cột bị trùng khác\n",
    "flist_remove = ['saldo_medio_var13_medio_ult1', 'delta_imp_reemb_var13_1y3', 'delta_imp_reemb_var17_1y3', \n",
    "                   'delta_imp_reemb_var33_1y3', 'delta_imp_trasp_var17_in_1y3', 'delta_imp_trasp_var17_out_1y3',\n",
    "                   'delta_imp_trasp_var33_in_1y3', 'delta_imp_trasp_var33_out_1y3']\n",
    "train.drop(flist_remove, axis=1, inplace=True)\n",
    "test.drop(flist_remove, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b39ce5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Chuẩn hóa các giá trị thuộc tính\n",
    "def normalize_features(train, test):\n",
    "    flist = [x for x in train.columns if not x in ['ID','TARGET']]\n",
    "    for f in flist:\n",
    "        if train[f].max() == 9999999999.0:\n",
    "            fmax = train.loc[train[f]<9999999999.0, f].max()\n",
    "            train.loc[train[f]==9999999999.0, f] = fmax + 1\n",
    "\n",
    "        if len(train.loc[train[f]<0, f].value_counts()) == 1:\n",
    "            train.loc[train[f]<0, f] = -1.0\n",
    "            test.loc[test[f]<0, f] = -1.0\n",
    "            fmax = max(np.max(train[f]), np.max(test[f]))\n",
    "            if fmax > 0:\n",
    "                train.loc[train[f]>0, f] = 1.0*train.loc[train[f]>0, f]/fmax\n",
    "                test.loc[test[f]>0, f] = 1.0*test.loc[test[f]>0, f]/fmax\n",
    "\n",
    "        if len(train.loc[train[f]<0, f]) == 0:\n",
    "            fmax = max(np.max(train[f]), np.max(test[f]))\n",
    "            if fmax > 0:\n",
    "                train.loc[train[f]>0, f] = 1.0*train.loc[train[f]>0, f]/fmax\n",
    "                test.loc[test[f]>0, f] = 1.0*test.loc[test[f]>0, f]/fmax\n",
    "\n",
    "        if len(train.loc[train[f]<0, f].value_counts()) > 1:\n",
    "            fmax = max(np.max(train[f]), np.max(test[f]))\n",
    "            if fmax > 0:\n",
    "                train[f] = 1.0*train[f]/fmax\n",
    "                test[f] = 1.0*test[f]/fmax\n",
    "\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19d503c",
   "metadata": {},
   "source": [
    "#### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed2acc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#thuộc tính t_SNE\n",
    "np.random.seed(12324)\n",
    "train_tsne, test_tsne = add_features(train, test, ['SumZeros'])\n",
    "\n",
    "flist = [x for x in train_tsne.columns if not x in ['ID','TARGET']]\n",
    "\n",
    "X = train_tsne[flist].append(test[flist], ignore_index=True).values.astype('float64')\n",
    "svd = TruncatedSVD(n_components=30)\n",
    "X_svd = svd.fit_transform(X)\n",
    "X_scaled = StandardScaler().fit_transform(X_svd)\n",
    "feats_tsne = TSNE(n_components=2, random_state=0).fit_transform(X_scaled)\n",
    "feats_tsne = pd.DataFrame(feats_tsne, columns=['tsne1', 'tsne2'])\n",
    "feats_tsne['ID'] = train_tsne[['ID']].append(test_tsne[['ID']], ignore_index=True)['ID'].values\n",
    "train_tsne = pd.merge(train_tsne, feats_tsne, on='ID', how='left')\n",
    "test_tsne = pd.merge(test_tsne, feats_tsne, on='ID', how='left')\n",
    "\n",
    "feat = train[['ID', 'tsne1', 'tsne2']].append(test[['ID', 'tsne1', 'tsne2']], ignore_index=True)\n",
    "feat.to_csv(OUTPUT_PATH + 'tsne_feats.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff2a755",
   "metadata": {},
   "outputs": [],
   "source": [
    "#thuộc tính PCA\n",
    "train_pca, test_pca = add_features(train, test, ['SumZeros'])\n",
    "\n",
    "flist = [x for x in train_pca.columns if not x in ['ID','TARGET']]\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "x_train_projected = pca.fit_transform(normalize(train_pca[flist], axis=0))\n",
    "x_test_projected = pca.transform(normalize(test_pca[flist], axis=0))\n",
    "train_pca.insert(1, 'PCAOne', x_train_projected[:, 0])\n",
    "train_pca.insert(1, 'PCATwo', x_train_projected[:, 1])\n",
    "test_pca.insert(1, 'PCAOne', x_test_projected[:, 0])\n",
    "test_pca.insert(1, 'PCATwo', x_test_projected[:, 1])\n",
    "pca_feats = train_pca[['ID', 'PCAOne', 'PCATwo']].append(test_pca[['ID', 'PCAOne', 'PCATwo']], ignore_index=True)\n",
    "pca_feats.to_csv(OUTPUT_PATH + 'dmitry_pca_feats.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b68446f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#thuộc tính k-means\n",
    "train_k, test_k = add_features(train_k, test, ['SumZeros'])\n",
    "train_k, test_k = normalize_features(train_k, test)\n",
    "\n",
    "flist = [x for x in train_k.columns if not x in ['ID','TARGET']]\n",
    "\n",
    "flist_kmeans = []\n",
    "for ncl in range(2,11):\n",
    "    cls = KMeans(n_clusters=ncl)\n",
    "    cls.fit_predict(train_k[flist].values)\n",
    "    train_k['kmeans_cluster'+str(ncl)] = cls.predict(train_k[flist].values)\n",
    "    test_k['kmeans_cluster'+str(ncl)] = cls.predict(test_k[flist].values)\n",
    "    flist_kmeans.append('kmeans_cluster'+str(ncl))\n",
    "\n",
    "train[['ID']+flist_kmeans].append(test[['ID']+flist_kmeans], ignore_index=True).to_csv(OUTPUT_PATH + 'kmeans_feats.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "798d0228",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LL = (30.yTB + yG)/(30 + |G|)\n",
    "def add_likelihood_feature(fname, train_likeli, test_likeli, flist):\n",
    "    tt_likeli = pd.DataFrame()\n",
    "    np.random.seed(1232345)\n",
    "    #Chia fold để tính toán các giá trị\n",
    "    #Tập test được điền theo tập train\n",
    "    skf = StratifiedKFold(train_likeli['TARGET'].values, n_folds=5, shuffle=True, random_state=21387)\n",
    "    for train_index, test_index in skf:\n",
    "        ids = train_likeli['ID'].values[train_index]\n",
    "        train_fold = train_likeli.loc[train_likeli['ID'].isin(ids)].copy()\n",
    "        test_fold = train_likeli.loc[~train_likeli['ID'].isin(ids)].copy()\n",
    "        global_avg = np.mean(train_fold['TARGET'].values)\n",
    "        feats_likeli = train_fold.groupby(fname)['TARGET'].agg({'sum': np.sum, 'count': len}).reset_index()\n",
    "        feats_likeli[fname + '_likeli'] = (feats_likeli['sum'] + 30.0*global_avg)/(feats_likeli['count']+30.0)\n",
    "        test_fold = pd.merge(test_fold, feats_likeli[[fname, fname + '_likeli']], on=fname, how='left')\n",
    "        test_fold[fname + '_likeli'] = test_fold[fname + '_likeli'].fillna(global_avg)\n",
    "        tt_likeli = tt_likeli.append(test_fold[['ID', fname + '_likeli']], ignore_index=True)\n",
    "    train_likeli = pd.merge(train_likeli, tt_likeli, on='ID', how='left')\n",
    "    \n",
    "    global_avg = np.mean(train_likeli['TARGET'].values)\n",
    "    feats_likeli = train_likeli.groupby(fname)['TARGET'].agg({'sum': np.sum, 'count': len}).reset_index()\n",
    "    feats_likeli[fname + '_likeli'] = (feats_likeli['sum'] + 30.0*global_avg)/(feats_likeli['count']+30.0)\n",
    "    test_likeli = pd.merge(test_likeli, feats_likeli[[fname, fname + '_likeli']], on=fname, how='left')\n",
    "    test_likeli[fname + '_likeli'] = test_likeli[fname + '_likeli'].fillna(global_avg)\n",
    "    return train_likeli, test_likeli, flist + [fname + '_likeli']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1254fc91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_features(train, test, features):\n",
    "    flist = [x for x in train.columns if not x in ['ID','TARGET']]\n",
    "    if 'SumZeros' in features:\n",
    "        train.insert(1, 'SumZeros', (train[flist] == 0).astype(int).sum(axis=1))\n",
    "        test.insert(1, 'SumZeros', (test[flist] == 0).astype(int).sum(axis=1))\n",
    "    flist = [x for x in train.columns if not x in ['ID','TARGET']]\n",
    "\n",
    "    if 'tsne' in features:\n",
    "        tsne_feats = pd.read_csv(OUTPUT_PATH + 'features/tsne_feats.csv')\n",
    "        train = pd.merge(train, tsne_feats, on='ID', how='left')\n",
    "        test = pd.merge(test, tsne_feats, on='ID', how='left')\n",
    "\n",
    "    if 'pca' in features:\n",
    "        pca_feats = pd.read_csv(OUTPUT_PATH + 'features/dmitry_pca_feats.csv')\n",
    "        train = pd.merge(train, pca_feats, on='ID', how='left')\n",
    "        test = pd.merge(test, pca_feats, on='ID', how='left')\n",
    "\n",
    "    if 'kmeans' in features:\n",
    "        kmeans_feats = pd.read_csv(OUTPUT_PATH + 'features/kmeans_feats.csv')\n",
    "        train = pd.merge(train, kmeans_feats, on='ID', how='left')\n",
    "        test = pd.merge(test, kmeans_feats, on='ID', how='left')\n",
    "\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b38de3",
   "metadata": {},
   "source": [
    "#### Mô hình"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eec7745",
   "metadata": {},
   "source": [
    "#####  FTRL2 (Follow the Regularized Leader)\n",
    "Sử dụng dữ liệu gốc, Sumzeros và Likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9602f088",
   "metadata": {},
   "source": [
    "#####  Regularized Greedy Forest 3\n",
    "Sử dụng dữ liệu gốc, Sumzeros, PCA và Likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e39fc8f",
   "metadata": {},
   "source": [
    "#####  Regularized Greedy Forest 5\n",
    "Sử dụng dữ liệu gốc, Sumzeros, tSNE và Likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7d96a2",
   "metadata": {},
   "source": [
    "#####  Regularized Greedy Forest 6\n",
    "Sử dụng dữ liệu gốc, Sumzeros, K-means và Likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6273a0",
   "metadata": {},
   "source": [
    "#####  Adaboost classifier\n",
    "Sử dụng dữ liệu gốc, Sumzeros, PCA và Likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef4931e",
   "metadata": {},
   "source": [
    "#####  XGBOOST\n",
    "Sử dụng dữ liệu gốc, Sumzeros, PCA và Likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ae5f85",
   "metadata": {},
   "source": [
    "### Nhóm mô hình Ikki Tanaka"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5595d487",
   "metadata": {},
   "source": [
    "#### Tiền xử lý dữ liệu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182132f2",
   "metadata": {},
   "source": [
    "#### Mô hình"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932328f5",
   "metadata": {},
   "source": [
    "### Nhóm mô hình Darius Barusauskas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec2c73c",
   "metadata": {},
   "source": [
    "#### Tiền xử lý dữ liệu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a782168",
   "metadata": {},
   "source": [
    "#### Mô hình"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c8e41c",
   "metadata": {},
   "source": [
    "### Nhóm mô hình Marios Michailidis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e33b815",
   "metadata": {},
   "source": [
    "#### Tiền xử lý dữ liệu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334525a9",
   "metadata": {},
   "source": [
    "#### Mô hình"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e92a6d",
   "metadata": {},
   "source": [
    "### Nhóm mô hình Mathias Muller"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ab9072",
   "metadata": {},
   "source": [
    "#### Tiền xử lý dữ liệu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7b6d25",
   "metadata": {},
   "source": [
    "#### Mô hình"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae45e1e",
   "metadata": {},
   "source": [
    "## Tài liệu tham khảo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9701b2bd",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
