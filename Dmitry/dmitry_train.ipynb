{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d74719ef",
   "metadata": {},
   "source": [
    "# Import thư viện và tải dữ liệu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18db85bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from scipy.stats.stats import pearsonr\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import Normalizer, PolynomialFeatures, MinMaxScaler\n",
    "from datetime import date\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "import xgboost as xgb\n",
    "import os\n",
    "import glob\n",
    "import sys, getopt, re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02442e08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-01-07 20:03:39--  https://github.com/linh0303052/AplliedDSGroup11/raw/main/data.tar.gz\n",
      "Resolving github.com (github.com)... 20.205.243.166\n",
      "Connecting to github.com (github.com)|20.205.243.166|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://raw.githubusercontent.com/linh0303052/AplliedDSGroup11/main/data.tar.gz [following]\n",
      "--2022-01-07 20:03:39--  https://raw.githubusercontent.com/linh0303052/AplliedDSGroup11/main/data.tar.gz\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 12633830 (12M) [application/octet-stream]\n",
      "Saving to: ‘data.tar.gz’\n",
      "\n",
      "data.tar.gz         100%[===================>]  12.05M  8.57MB/s    in 1.4s    \n",
      "\n",
      "2022-01-07 20:03:40 (8.57 MB/s) - ‘data.tar.gz’ saved [12633830/12633830]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://github.com/linh0303052/AplliedDSGroup11/raw/main/data.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be18c238",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/\n",
      "data/input/\n",
      "data/input/5fold_20times.csv\n",
      "data/input/sample_submission.csv\n",
      "data/input/test.csv\n",
      "data/input/train.csv\n",
      "data/output/\n",
      "data/output/features/\n",
      "data/output/features/dmitry_pca_feats.csv\n",
      "data/output/features/kmeans_feats.csv\n",
      "data/output/features/tsne_feats.csv\n"
     ]
    }
   ],
   "source": [
    "!tar -xzvf data.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e37055",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('./data/input/train.csv')\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85502c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('./data/input/test.csv')\n",
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5331726",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_PATH = './data/input/'\n",
    "OUTPUT_PATH = './data/output/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8510fe",
   "metadata": {},
   "source": [
    "# Tiền xử lý dữ liệu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6cffbab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#xử lý các giá trị đặc biệt, thay thế nó bằng giá trị NA (-999.0)\n",
    "def process_base(train, test):\n",
    "    train.loc[(train['var38']>117310.979) & (train['var38']<117310.98), 'var38'] = -999.0\n",
    "    test.loc[(test['var38']>117310.979) & (test['var38']<117310.98), 'var38'] = -999.0\n",
    "\n",
    "    train.loc[train['var3']==-999999, 'var3'] = -999.0\n",
    "    test.loc[test['var3']==-999999, 'var3'] = -999.0\n",
    "\n",
    "    for f in ['imp_op_var40_comer_ult1', 'imp_op_var40_efect_ult3', 'imp_op_var41_comer_ult3', 'imp_sal_var16_ult1']:\n",
    "        train.loc[train[f]==0.0, f] = -999.0\n",
    "        test.loc[test[f]==0.0, f] = -999.0\n",
    "\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7b6013f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_sparse(train, test):\n",
    "    flist = [x for x in train.columns if not x in ['ID','TARGET']]\n",
    "    for f in flist:\n",
    "        if len(np.unique(train[f]))<2:\n",
    "            train.drop(f, axis=1, inplace=True)\n",
    "            test.drop(f, axis=1, inplace=True)\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "12640965",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_duplicated(train, test):    \n",
    "    #Loại bỏ var6 vì nó trùng với var29\n",
    "    flist = [x for x in train.columns if not x in ['ID','TARGET']]            \n",
    "    train.drop([x for x in flist if 'var6' in x], axis=1, inplace=True)\n",
    "    test.drop([x for x in flist if 'var6' in x], axis=1, inplace=True)\n",
    "\n",
    "    #Loại bỏ các thuộc tính có chứa _0 vì nó bị trùng với cột có chứa _1 theo ngay sau\n",
    "    flist = [x for x in train.columns if not x in ['ID','TARGET']]        \n",
    "    flist_remove = []\n",
    "    for i in range(len(flist)-1):\n",
    "        v = train[flist[i]].values\n",
    "        for j in range(i+1, len(flist)):\n",
    "            if np.array_equal(v, train[flist[j]].values):\n",
    "                if '_0' in flist[j]:\n",
    "                    flist_remove.append(flist[j])\n",
    "                elif  '_0' in flist[i]:\n",
    "                    flist_remove.append(flist[i])\n",
    "    train.drop(flist_remove, axis=1, inplace=True)\n",
    "    test.drop(flist_remove, axis=1, inplace=True)\n",
    "\n",
    "    #Loại bỏ các cột bị trùng khác\n",
    "    flist_remove = ['saldo_medio_var13_medio_ult1', 'delta_imp_reemb_var13_1y3', 'delta_imp_reemb_var17_1y3', \n",
    "                       'delta_imp_reemb_var33_1y3', 'delta_imp_trasp_var17_in_1y3', 'delta_imp_trasp_var17_out_1y3',\n",
    "                       'delta_imp_trasp_var33_in_1y3', 'delta_imp_trasp_var33_out_1y3']\n",
    "    train.drop(flist_remove, axis=1, inplace=True)\n",
    "    test.drop(flist_remove, axis=1, inplace=True)\n",
    "    \n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2a24edbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Chuẩn hóa các giá trị thuộc tính\n",
    "def normalize_features(train, test):\n",
    "    flist = [x for x in train.columns if not x in ['ID','TARGET']]\n",
    "    for f in flist:\n",
    "        if train[f].max() == 9999999999.0:\n",
    "            fmax = train.loc[train[f]<9999999999.0, f].max()\n",
    "            train.loc[train[f]==9999999999.0, f] = fmax + 1\n",
    "\n",
    "        if len(train.loc[train[f]<0, f].value_counts()) == 1:\n",
    "            train.loc[train[f]<0, f] = -1.0\n",
    "            test.loc[test[f]<0, f] = -1.0\n",
    "            fmax = max(np.max(train[f]), np.max(test[f]))\n",
    "            if fmax > 0:\n",
    "                train.loc[train[f]>0, f] = 1.0*train.loc[train[f]>0, f]/fmax\n",
    "                test.loc[test[f]>0, f] = 1.0*test.loc[test[f]>0, f]/fmax\n",
    "\n",
    "        if len(train.loc[train[f]<0, f]) == 0:\n",
    "            fmax = max(np.max(train[f]), np.max(test[f]))\n",
    "            if fmax > 0:\n",
    "                train.loc[train[f]>0, f] = 1.0*train.loc[train[f]>0, f]/fmax\n",
    "                test.loc[test[f]>0, f] = 1.0*test.loc[test[f]>0, f]/fmax\n",
    "\n",
    "        if len(train.loc[train[f]<0, f].value_counts()) > 1:\n",
    "            fmax = max(np.max(train[f]), np.max(test[f]))\n",
    "            if fmax > 0:\n",
    "                train[f] = 1.0*train[f]/fmax\n",
    "                test[f] = 1.0*test[f]/fmax\n",
    "\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a85719",
   "metadata": {},
   "source": [
    "# Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b918d496",
   "metadata": {},
   "outputs": [],
   "source": [
    "#thuộc tính t_SNE\n",
    "np.random.seed(12324)\n",
    "train_tsne, test_tsne = add_features(train, test, ['SumZeros'])\n",
    "\n",
    "flist = [x for x in train_tsne.columns if not x in ['ID','TARGET']]\n",
    "\n",
    "X = train_tsne[flist].append(test_tsne[flist], ignore_index=True).values.astype('float64')\n",
    "svd = TruncatedSVD(n_components=30)\n",
    "X_svd = svd.fit_transform(X)\n",
    "X_scaled = StandardScaler().fit_transform(X_svd)\n",
    "feats_tsne = TSNE(n_components=2, random_state=0).fit_transform(X_scaled)\n",
    "feats_tsne = pd.DataFrame(feats_tsne, columns=['tsne1', 'tsne2'])\n",
    "feats_tsne['ID'] = train_tsne[['ID']].append(test_tsne[['ID']], ignore_index=True)['ID'].values\n",
    "train_tsne = pd.merge(train_tsne, feats_tsne, on='ID', how='left')\n",
    "test_tsne = pd.merge(test_tsne, feats_tsne, on='ID', how='left')\n",
    "\n",
    "feat = train_tsne[['ID', 'tsne1', 'tsne2']].append(test_tsne[['ID', 'tsne1', 'tsne2']], ignore_index=True)\n",
    "feat.to_csv(OUTPUT_PATH + 'tsne_feats.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c534d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "#thuộc tính PCA\n",
    "train_pca, test_pca = add_features(train, test, ['SumZeros'])\n",
    "\n",
    "flist = [x for x in train_pca.columns if not x in ['ID','TARGET']]\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "x_train_projected = pca.fit_transform(normalize(train_pca[flist], axis=0))\n",
    "x_test_projected = pca.transform(normalize(test_pca[flist], axis=0))\n",
    "train_pca.insert(1, 'PCAOne', x_train_projected[:, 0])\n",
    "train_pca.insert(1, 'PCATwo', x_train_projected[:, 1])\n",
    "test_pca.insert(1, 'PCAOne', x_test_projected[:, 0])\n",
    "test_pca.insert(1, 'PCATwo', x_test_projected[:, 1])\n",
    "pca_feats = train_pca[['ID', 'PCAOne', 'PCATwo']].append(test_pca[['ID', 'PCAOne', 'PCATwo']], ignore_index=True)\n",
    "pca_feats.to_csv(OUTPUT_PATH + 'dmitry_pca_feats.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0597df4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#thuộc tính k-means\n",
    "train_k, test_k = add_features(train, test, ['SumZeros'])\n",
    "train_k, test_k = normalize_features(train_k, test_k)\n",
    "\n",
    "flist = [x for x in train_k.columns if not x in ['ID','TARGET']]\n",
    "\n",
    "flist_kmeans = []\n",
    "for ncl in range(2,11):\n",
    "    cls = KMeans(n_clusters=ncl)\n",
    "    cls.fit_predict(train_k[flist].values)\n",
    "    train_k['kmeans_cluster'+str(ncl)] = cls.predict(train_k[flist].values)\n",
    "    test_k['kmeans_cluster'+str(ncl)] = cls.predict(test_k[flist].values)\n",
    "    flist_kmeans.append('kmeans_cluster'+str(ncl))\n",
    "\n",
    "train[['ID']+flist_kmeans].append(test[['ID']+flist_kmeans], ignore_index=True).to_csv(OUTPUT_PATH + 'kmeans_feats.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4770e292",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Thuộc tính LL = (30.yTB + yG)/(30 + |G|)\n",
    "def add_likelihood_feature(fname, train_likeli, test_likeli, flist):\n",
    "    tt_likeli = pd.DataFrame()\n",
    "    np.random.seed(1232345)\n",
    "    #Chia fold để tính toán các giá trị\n",
    "    #Tập test được điền theo tập train\n",
    "    skf = StratifiedKFold(train_likeli['TARGET'].values, n_folds=5, shuffle=True, random_state=21387)\n",
    "    for train_index, test_index in skf:\n",
    "        ids = train_likeli['ID'].values[train_index]\n",
    "        train_fold = train_likeli.loc[train_likeli['ID'].isin(ids)].copy()\n",
    "        test_fold = train_likeli.loc[~train_likeli['ID'].isin(ids)].copy()\n",
    "        global_avg = np.mean(train_fold['TARGET'].values)\n",
    "        feats_likeli = train_fold.groupby(fname)['TARGET'].agg({'sum': np.sum, 'count': len}).reset_index()\n",
    "        feats_likeli[fname + '_likeli'] = (feats_likeli['sum'] + 30.0*global_avg)/(feats_likeli['count']+30.0)\n",
    "        test_fold = pd.merge(test_fold, feats_likeli[[fname, fname + '_likeli']], on=fname, how='left')\n",
    "        test_fold[fname + '_likeli'] = test_fold[fname + '_likeli'].fillna(global_avg)\n",
    "        tt_likeli = tt_likeli.append(test_fold[['ID', fname + '_likeli']], ignore_index=True)\n",
    "    train_likeli = pd.merge(train_likeli, tt_likeli, on='ID', how='left')\n",
    "    \n",
    "    global_avg = np.mean(train_likeli['TARGET'].values)\n",
    "    feats_likeli = train_likeli.groupby(fname)['TARGET'].agg({'sum': np.sum, 'count': len}).reset_index()\n",
    "    feats_likeli[fname + '_likeli'] = (feats_likeli['sum'] + 30.0*global_avg)/(feats_likeli['count']+30.0)\n",
    "    test_likeli = pd.merge(test_likeli, feats_likeli[[fname, fname + '_likeli']], on=fname, how='left')\n",
    "    test_likeli[fname + '_likeli'] = test_likeli[fname + '_likeli'].fillna(global_avg)\n",
    "    return train_likeli, test_likeli, flist + [fname + '_likeli']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d3a6b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_features(train, test, features):\n",
    "    flist = [x for x in train.columns if not x in ['ID','TARGET']]\n",
    "    if 'SumZeros' in features:\n",
    "        train.insert(1, 'SumZeros', (train[flist] == 0).astype(int).sum(axis=1))\n",
    "        test.insert(1, 'SumZeros', (test[flist] == 0).astype(int).sum(axis=1))\n",
    "    flist = [x for x in train.columns if not x in ['ID','TARGET']]\n",
    "\n",
    "    if 'tsne' in features:\n",
    "        tsne_feats = pd.read_csv(OUTPUT_PATH + 'features/tsne_feats.csv')\n",
    "        train = pd.merge(train, tsne_feats, on='ID', how='left')\n",
    "        test = pd.merge(test, tsne_feats, on='ID', how='left')\n",
    "\n",
    "    if 'pca' in features:\n",
    "        pca_feats = pd.read_csv(OUTPUT_PATH + 'features/dmitry_pca_feats.csv')\n",
    "        train = pd.merge(train, pca_feats, on='ID', how='left')\n",
    "        test = pd.merge(test, pca_feats, on='ID', how='left')\n",
    "\n",
    "    if 'kmeans' in features:\n",
    "        kmeans_feats = pd.read_csv(OUTPUT_PATH + 'features/kmeans_feats.csv')\n",
    "        train = pd.merge(train, kmeans_feats, on='ID', how='left')\n",
    "        test = pd.merge(test, kmeans_feats, on='ID', how='left')\n",
    "\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70bb0d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_predict_adaboost_classifier(X_train, y_train, X_test):\n",
    "    clf = AdaBoostClassifier(n_estimators=300, learning_rate=0.1, random_state=32934)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict_proba(X_test)[:,1]\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3fcdb0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_predict_xgboost_bugged(X_train, y_train, X_test):\n",
    "    param = {}\n",
    "    param['objective'] = 'binary:logistic'\n",
    "    param['eta'] = 0.02\n",
    "    param['max_depth'] = 5\n",
    "    param['eval_metric'] = 'auc'\n",
    "    param['silent'] = 1\n",
    "    param['nthread'] = 6\n",
    "    param['gamma'] = 1.0\n",
    "    param['min_child_weight'] = 5\n",
    "    param['subsample'] = 0.8\n",
    "    param['colsample_bytree'] = 1.0\n",
    "    param['colsample_bylevel'] = 0.7\n",
    "    num_round = 500\n",
    "\n",
    "    y_pred = [0.0]*len(X_test)\n",
    "    for seed in [123089, 21324, 324003, 450453, 120032]:\n",
    "        param['seed'] = seed\n",
    "        plst = list(param.items())\n",
    "        xgmat_train = xgb.DMatrix(X_train, label=y_train, missing = -999.0)\n",
    "        xgmat_test = xgb.DMatrix(X_test, missing = -999.0)\n",
    "        bst = xgb.train(plst, xgmat_train, num_round)\n",
    "        y_pred = y_pred + bst.predict( xgmat_test )\n",
    "    y_pred = y_pred/5.0\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e5feffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_predict_ftrl(X_train, y_train, X_test):\n",
    "    train_file = '../data/output-ftrl/train_ftrl.csv'\n",
    "    test_file = '../data/output-ftrl/test_ftrl.csv'\n",
    "    pred_file = '../data/output-ftrl/pred_ftrl.csv'\n",
    "\n",
    "    train_csv = pd.DataFrame(X_train)\n",
    "    train_csv['TARGET'] = y_train\n",
    "    train_csv['ID'] = [x for x in range(1, len(train_csv)+1)]\n",
    "    train_csv.to_csv(train_file, index=False)\n",
    "\n",
    "    test_csv = pd.DataFrame(X_test)\n",
    "    test_csv['ID'] = [x for x in range(1, len(test_csv)+1)]\n",
    "    test_csv.to_csv(test_file, index=False)\n",
    "\n",
    "    non_factor_cols = \"''\"\n",
    "    non_feature_cols = \"''\"\n",
    "    text_cols = \"''\"\n",
    "\n",
    "    os.system('pypy ftrl.py' +\n",
    "              ' --alpha ' + str(0.06) +\n",
    "              ' --beta ' + str(1.0) +\n",
    "              ' --L1 ' + str(0.01) +\n",
    "              ' --L2 ' + str(1.0) +\n",
    "              ' --epoch ' + str(3) +\n",
    "              ' --train ' + train_file +\n",
    "              ' --test ' + test_file +\n",
    "              ' --submission ' + pred_file +\n",
    "              ' --non_feature_cols ' + non_feature_cols +\n",
    "              ' --non_factor_cols ' + non_factor_cols + \n",
    "              ' --text_cols ' + text_cols)\n",
    "\n",
    "    y_pred = pd.read_csv(pred_file)['PRED'].values\n",
    "    filelist = glob.glob(\"../data/output-ftrl/*.*\")\n",
    "    for f in filelist:\n",
    "        os.remove(f)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7050419",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_PATH = '../data/input/'\n",
    "OUTPUT_PATH = '../data/output/'\n",
    "\n",
    "MODELS_ALL = ['ftrl2', 'adaboost_classifier', 'xgboost']\n",
    "FEATURES_ALL = [['SumZeros', 'likeli'], \n",
    "                ['SumZeros', 'pca', 'likeli'],\n",
    "                ['SumZeros', 'pca', 'likeli']]\n",
    "\n",
    "train = pd.read_csv(INPUT_PATH + 'train.csv')\n",
    "test = pd.read_csv(INPUT_PATH + 'test.csv')\n",
    "preds_all = train[['ID']].append(test[['ID']], ignore_index=True).copy()\n",
    "for imod in range(len(MODELS_ALL)):\n",
    "    MODEL = MODELS_ALL[imod]\n",
    "    FEATURES = FEATURES_ALL[imod]\n",
    "    print 'Training ' + MODEL + '...'\n",
    "\n",
    "    train = pd.read_csv(INPUT_PATH + 'train.csv')\n",
    "    test = pd.read_csv(INPUT_PATH + 'test.csv')\n",
    "    id_fold = pd.read_csv(INPUT_PATH+'5fold_20times.csv')\n",
    "    id_fold['ID'] = train['ID'].values\n",
    "\n",
    "    train, test = process_base(train, test)\n",
    "    train, test = drop_sparse(train, test)\n",
    "    train, test = drop_duplicated(train, test)\n",
    "    train, test = add_features(train, test, FEATURES)\n",
    "\n",
    "    flist = [x for x in train.columns if not x in ['ID','TARGET']]\n",
    "\n",
    "    preds_model = pd.DataFrame()\n",
    "    for it in range(1, 21):\n",
    "        print 'Processing iteration ' + str(it) + '...'   \n",
    "        it_id_fold = id_fold[['ID', 'set'+str(it)]]\n",
    "        it_id_fold.columns = ['ID', 'FOLD']\n",
    "        if 'FOLD' in train.columns:\n",
    "            train.drop('FOLD', axis=1, inplace=True)\n",
    "        train = pd.merge(train, it_id_fold, on='ID', how='left')\n",
    "        aucs = []\n",
    "        for fold in range(5):\n",
    "            train_split = train.query('FOLD != @fold').copy().reset_index(drop=True)\n",
    "            y_train = train_split['TARGET'].values\n",
    "            val_split = train.query('FOLD == @fold').copy().reset_index(drop=True)\n",
    "            test_split = val_split[['ID']+flist].append(test[['ID']+flist], ignore_index=True)\n",
    "            ids_val = val_split['ID'].values\n",
    "\n",
    "            if 'likeli' in FEATURES:\n",
    "                train_split, test_split, flist1 = add_likelihood_feature('saldo_var13', train_split, test_split, flist)\n",
    "            else:\n",
    "                flist1 = flist\n",
    "            \n",
    "            X_train = train_split[flist1].values\n",
    "            y_train = train_split['TARGET'].values\n",
    "            X_test = test_split[flist1].values\n",
    "\n",
    "\n",
    "            if MODEL == 'xgboost':\n",
    "                y_pred = train_predict_xgboost_bugged(X_train, y_train, X_test)            \n",
    "\n",
    "            if MODEL == 'adaboost_classifier':\n",
    "                y_pred = train_predict_adaboost_classifier(X_train, y_train, X_test)\n",
    "\n",
    "            if 'ftrl' in MODEL:\n",
    "                y_pred = train_predict_ftrl(X_train, y_train, X_test)\n",
    "            \n",
    "            preds = pd.DataFrame()\n",
    "            preds['ID'] = test_split['ID'].values\n",
    "            preds['FOLD'] = fold\n",
    "            preds['ITER'] = it\n",
    "            preds[MODEL] = y_pred\n",
    "            preds_model = preds_model.append(preds, ignore_index=True)\n",
    "\n",
    "            preds = preds.loc[preds['ID'].isin(ids_val)].copy()\n",
    "            preds = pd.merge(preds, train[['ID', 'TARGET']], on='ID', how='left')\n",
    "\n",
    "            fold_auc = auc(preds['TARGET'], preds[MODEL])\n",
    "            aucs.append(fold_auc)\n",
    "        print np.mean(aucs), np.std(aucs)\n",
    "\n",
    "    preds_model.loc[preds_model[MODEL]<0, MODEL] = 0.0\n",
    "    preds_model.loc[preds_model[MODEL]>1, MODEL] = 1.0\n",
    "    preds_model = preds_model.groupby(['ID', 'ITER'])[MODEL].mean().reset_index()\n",
    "    for it in range(1, 21):\n",
    "        preds_model.loc[preds_model['ITER']==it, MODEL] = preds_model.loc[preds_model['ITER']==it, MODEL].rank()\n",
    "    preds_model = preds_model.groupby('ID')[MODEL].mean().reset_index()\n",
    "    preds_model.columns = ['ID', 'dmitry_'+MODEL]\n",
    "    preds_all = pd.merge(preds_all, preds_model, on='ID', how='left')\n",
    "    preds_all.to_csv('all_models_temp.csv', index=False)\n",
    "\n",
    "preds_train = pd.merge(train[['ID']], preds_all, on='ID', how='left')\n",
    "preds_train.to_csv(OUTPUT_PATH + 'train/' + 'dmitry_train.csv', index=False)\n",
    "preds_test = pd.merge(test[['ID']], preds_all, on='ID', how='left')\n",
    "preds_test.to_csv(OUTPUT_PATH + 'test/' + 'dmitry_test.csv', index=False)\n",
    "print \"Done training!\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
